{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"10_Character-level Text Generation.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"LacGXqoRaJvD","colab_type":"text"},"source":["# Lecture 10. Character-level Text Generation"]},{"cell_type":"markdown","metadata":{"id":"Tpxy2c15aJvG","colab_type":"text"},"source":["## 10.1 Text 처리 \n","- 텍스트 데이터는 가장 흔한 시퀀스 형태의 데이터\n","- Document classification, sentiment analysis, Question answering 등에 활용 \n","- 텍스트 원본을 모형의 입력값으로 사용하지 못하기 때문에 텍스트를 수치형 텐서로 변환시키는 과정이 필요함\n","\n","- 텍스트 $\\rightarrow$ 토큰 $\\rightarrow$ 벡터 \n","![](figures/token.PNG)"]},{"cell_type":"markdown","metadata":{"id":"Se3MG9k-aJvJ","colab_type":"text"},"source":["### Tokenization\n","        \n","- 텍스트를 수치형 텐서로 변환하기 위해 나누는 단위\n","    - 각 단어를 하나의 벡터로 변환\n","            {“the”, “cat”, “sat”, “on”, “the”, “mat”, “.”}\n","    - 각 문자를 하나의 벡터로 변환\n","            {\"a\",\"c\",\"e\",\"h\",\"m\",\"n\",\"o\",\"s\",\"t\",\".\"}\n","    - n-gram(연속된 단어나 문자의 그룹)을 추출하여 벡터로 변환 \n","        - 2-grams\n","                {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n","        - 3-grams\n","                {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n","\n","- 토큰의 집합을 vocabulary, dictionary 라고 일컬음"]},{"cell_type":"markdown","metadata":{"id":"7GKJfC7haJvV","colab_type":"text"},"source":["### 단어와 문자의 인코딩\n","1. One-hot encoding\n","    - 토큰을 벡터화 하는 가장 기본적인 방법\n","    - 모든 단어에 고유한 정수 인덱스를 부여\n","    - 정수 인덱스를 vocabulary size 크기의 binary 벡터로 변환 "]},{"cell_type":"code","metadata":{"id":"gBQJFLT4aJvZ","colab_type":"code","colab":{}},"source":["# Toy example\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","text = \" \".join(samples)\n","\n","chars = set([c for c in text])\n","nb_chars = len(chars)\n","\n","char2index = dict((c, i) for i, c in enumerate(sorted(chars)))\n","index2char = dict((i, c) for i, c in enumerate(sorted(chars)))\n","\n","import numpy as np\n","max_length = 50\n","results = np.zeros((len(samples), max_length, len(char2index)))\n","for i, sample in enumerate(samples):\n","    for j, character in enumerate(sample):\n","        results[i, j, char2index[character]] = 1."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jP5QmKabaJvk","colab_type":"text"},"source":["2. Word embedding\n","    - One-hot encoding을 하게 되면 한 단어를 나타내는 벡터의 길이가 vocabulary size와 같기 때문에 일반적으로 매우 고차원이고 대부분이 0으로 채워져 있음: 비효율적\n","    - 0 또는 1로 채워진 고차원의 벡터 대신 실수값으로 채워져 있는 저차원 벡터로 표현하는 방법을 사용 \n","    - 저차원에 더 많은 정보를 저장할 수 있어 효율적임\n","    - Lecture 11에서 자세히 다룰 예정 \n","\n","![](figures/onehot_embed.PNG)"]},{"cell_type":"markdown","metadata":{"id":"naGqlseDaJvm","colab_type":"text"},"source":["## 10.2 Character-level text generation model\n","- 소설책의 텍스트를 학습하여 문장을 자동생성하는 모델\n","-  Alice's Adventures in Wonderland\n"]},{"cell_type":"markdown","metadata":{"id":"Wo33Co72aJvq","colab_type":"text"},"source":["      \n","    ... Alice was beginning to get very tired of sitting by her sister\n","    on the bank, and of having nothing to do:  once or twice she had\n","    peeped into the book her sister was reading, but it had no\n","    pictures or conversations in it, `and what is the use of a book,'\n","    thought Alice `without pictures or conversation?' ...\n","    \n","- 첫 10개의 문자를 입력하여 다음에 나타날 문자를 예측\n","\n","<img src=\"figures/text_gen.png\" width=\"40%\" align=\"left\">\n","<img src=\"figures/text_gen2.PNG\" width=\"30%\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EWBD-XHvaJvu","colab_type":"text"},"source":["### Preparing the data"]},{"cell_type":"markdown","metadata":{"id":"jzpK18iXaJvy","colab_type":"text"},"source":["- 텍스트를 한 줄씩 불러들여 소문자 변환, 빈 줄 삭제 등의 기본적인 전처리를 진행"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.197378Z","start_time":"2018-07-10T08:42:53.192152Z"},"id":"BeAafPf_aJv1","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","INPUT_FILE = \"/content/drive/My Drive/Deep_Learning_한국정보문화산업진흥원/2일차/data/alice_in_wonderland.txt\"\n","\n","fin = open(INPUT_FILE, 'rb') # 바이너리 파일을 읽기 모드로 오픈\n","\n","lines = []\n","i=0\n","for line in fin: # 파일을 한 줄씩 읽어들임\n","    line = line.strip().lower() # 공백을 제거하고 소문자로 변환\n","    line = line.decode(\"ascii\") # 디코딩하여 char로 변환\n","    if len(line) == 0: # 빈 줄 삭제\n","        continue\n","    lines.append(line)\n","fin.close()\n","\n","text = \" \".join(lines) # 모든 줄을 하나로 이어붙임"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpPG-10Ea3Dy","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33XVb4qQaJwF","colab_type":"text"},"source":["- 문자 수준의 One-hot encoding을 하기 위해 유일한 문자들의 집합인 vocabulary 생성"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.223841Z","start_time":"2018-07-10T08:42:53.212679Z"},"id":"ZpJXOjGBaJwH","colab_type":"code","colab":{}},"source":["chars = set([c for c in text])\n","nb_chars = len(chars)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqFly5qjaJwT","colab_type":"code","outputId":"62ca6dc4-3043-4106-edf8-c3cac0b2099e","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1575907821937,"user_tz":-540,"elapsed":574,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["nb_chars"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["45"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"odHya6t_aJwc","colab_type":"text"},"source":["* char와 index를 연결하는 lookup table 구축\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.223841Z","start_time":"2018-07-10T08:42:53.212679Z"},"id":"0AsrEm7HaJwj","colab_type":"code","colab":{}},"source":["char2index = dict((c, i) for i, c in enumerate(sorted(chars)))\n","index2char = dict((i, c) for i, c in enumerate(sorted(chars)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8QOVC1yaJwp","colab_type":"code","outputId":"a3817e7c-4aab-4717-a796-8c1d852dff39","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1575907824026,"user_tz":-540,"elapsed":568,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["char2index['a']"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"cEHaHqmlaJw0","colab_type":"code","outputId":"7374a4d9-da25-487c-a8b4-8bcd327c3363","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1575907826616,"user_tz":-540,"elapsed":741,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["index2char[12]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["':'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"t4P_e-xfaJxC","colab_type":"text"},"source":["- Input sequence와 output label 생성 \n","- SEQLEN: 다음 문자를 예측하기 위해 입력할 문자의 수 \n","- STEP: 몇 개씩 건너뛰며 window를 이동할 것인가? \n","- Ex: Input text= \"The sky was falling\"\n","    - The sky wa -> s\n","    - he sky was -> \" \"  \n","    - e sky was  -> f\n","    - sky was f -> a\n","    - sky was fa -> l"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.318734Z","start_time":"2018-07-10T08:42:53.225761Z"},"id":"vDmaDsLPaJxE","colab_type":"code","colab":{}},"source":["SEQLEN = 10\n","STEP = 1\n","\n","input_chars = []\n","label_chars = []\n","for i in range(0, len(text) - SEQLEN, STEP):\n","    input_chars.append(text[i:i + SEQLEN])\n","    label_chars.append(text[i + SEQLEN])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.326050Z","start_time":"2018-07-10T08:42:53.320449Z"},"id":"KGx_KeNRaJxJ","colab_type":"code","outputId":"b3fdb78d-e203-4229-c6fe-9299388d246c","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"ok","timestamp":1575907831026,"user_tz":-540,"elapsed":702,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["input_chars[0:10],label_chars[0:10]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([\"alice's ad\",\n","  \"lice's adv\",\n","  \"ice's adve\",\n","  \"ce's adven\",\n","  \"e's advent\",\n","  \"'s adventu\",\n","  's adventur',\n","  ' adventure',\n","  'adventures',\n","  'dventures '],\n"," ['v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i'])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Hy-IUN62aJxT","colab_type":"code","outputId":"02920b0e-4135-4930-f7cf-9c328fb74d22","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1575907834598,"user_tz":-540,"elapsed":1045,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["len(input_chars)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["143504"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"_k7M6--laJxf","colab_type":"text"},"source":["- 위에서 만든 input/output 문자 셋을 one-hot encoding으로 변환하여 모형에 입력 가능한 형태로 변환"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.945087Z","start_time":"2018-07-10T08:42:53.328270Z"},"id":"U2O9rFnPaJxj","colab_type":"code","colab":{}},"source":["X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n","y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n","\n","for i, input_char in enumerate(input_chars):\n","    for j, ch in enumerate(input_char):\n","        X[i, j, char2index[ch]] = 1\n","    y[i, char2index[label_chars[i]]] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RyKIH6xwaJxq","colab_type":"text"},"source":["- input과 output chars를 nb_chars 길이의 one-hot vector로 표현 \n","- input\n","    - (len(input_chars), SEQLEN, nb_chars)\n","    - SEQLEN 개 시점의 nb_chars 차원의 벡터가 input shape\n","- output\n","    - (len(input_chars), nb_chars)\n","    - (SEQLEN, nb_chars) 차원의 각 input에 대응하는 output label"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:53.952917Z","start_time":"2018-07-10T08:42:53.947986Z"},"id":"pI9FoV4KaJxr","colab_type":"code","outputId":"c9bdb4f2-5faa-4360-e2eb-2781b963ce22","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1575907840791,"user_tz":-540,"elapsed":899,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["print(input_chars[0])\n","print(X[0].shape)\n","print(y.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["alice's ad\n","(10, 45)\n","(143504, 45)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yQa2C-IEaJx-","colab_type":"text"},"source":["\n","### Building the network"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T08:42:54.273176Z","start_time":"2018-07-10T08:42:53.955909Z"},"id":"aB_Tm2imaJx_","colab_type":"code","outputId":"c9d29f92-7c65-403d-8723-340d2baa2dbb","colab":{"base_uri":"https://localhost:8080/","height":688}},"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from keras import optimizers\n","from keras import backend as K\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard \n","import time \n","\n","HIDDEN_SIZE = 32\n","BATCH_SIZE = 128\n","\n","K.clear_session()\n","model = Sequential()\n","model.add(LSTM(HIDDEN_SIZE, return_sequences=True, input_shape=(SEQLEN, nb_chars), activation='relu'))\n","model.add(LSTM(HIDDEN_SIZE, return_sequences=False, activation='relu'))\n","model.add(Dense(HIDDEN_SIZE, activation='relu'))\n","model.add(Dense(nb_chars, activation=\"softmax\"))\n","model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.adam(lr=0.001))\n","\n","now = time.strftime(\"%c\")\n","callbacks_list = [\n","    ModelCheckpoint(filepath='models/text_gen.h5', monitor='val_loss', save_best_only=True),\n","    TensorBoard(log_dir='logs/text_generation/'+now),\n","    EarlyStopping(monitor='val_loss',patience=3)\n","]\n","#model.fit(X, y, batch_size=BATCH_SIZE, epochs=100, validation_split=0.2, callbacks=callbacks_list)\n","model.fit(X, y, batch_size=BATCH_SIZE, epochs=100, validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 114803 samples, validate on 28701 samples\n","Epoch 1/100\n","114803/114803 [==============================] - 23s 199us/step - loss: 2.9295 - val_loss: 2.5682\n","Epoch 2/100\n","114803/114803 [==============================] - 22s 188us/step - loss: 2.3606 - val_loss: 2.3063\n","Epoch 3/100\n","114803/114803 [==============================] - 22s 189us/step - loss: 2.1768 - val_loss: 2.1462\n","Epoch 4/100\n","114803/114803 [==============================] - 22s 188us/step - loss: 2.0910 - val_loss: 2.0707\n","Epoch 5/100\n","114803/114803 [==============================] - 22s 190us/step - loss: 2.0314 - val_loss: 2.0211\n","Epoch 6/100\n","114803/114803 [==============================] - 21s 184us/step - loss: 1.9843 - val_loss: 2.0007\n","Epoch 7/100\n","114803/114803 [==============================] - 20s 175us/step - loss: 1.9423 - val_loss: 1.9532\n","Epoch 8/100\n","114803/114803 [==============================] - 21s 181us/step - loss: 1.9071 - val_loss: 1.9238\n","Epoch 9/100\n","114803/114803 [==============================] - 21s 183us/step - loss: 1.8771 - val_loss: 1.9038\n","Epoch 10/100\n","114803/114803 [==============================] - 21s 184us/step - loss: 1.8490 - val_loss: 1.8933\n","Epoch 11/100\n","114803/114803 [==============================] - 21s 186us/step - loss: 1.8263 - val_loss: 1.8658\n","Epoch 12/100\n","114803/114803 [==============================] - 21s 186us/step - loss: 1.8049 - val_loss: 1.8532\n","Epoch 13/100\n","114803/114803 [==============================] - 21s 184us/step - loss: 1.7859 - val_loss: 1.8508\n","Epoch 14/100\n","114803/114803 [==============================] - 21s 185us/step - loss: 1.7700 - val_loss: 1.8384\n","Epoch 15/100\n","114803/114803 [==============================] - 21s 184us/step - loss: 1.7556 - val_loss: 1.8055\n","Epoch 16/100\n","114803/114803 [==============================] - 22s 188us/step - loss: 1.7390 - val_loss: 1.8032\n","Epoch 17/100\n","114803/114803 [==============================] - 21s 187us/step - loss: 1.7258 - val_loss: 1.7970\n","Epoch 18/100\n","114176/114803 [============================>.] - ETA: 0s - loss: 1.7127"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"czP_btu1aJyE","colab_type":"text"},"source":["\n","### Prediction\n","\n","- random seed 선택 (10개의 char로 이루어진 부분 문장)\n","- 다음번 문자로 나타날 확률이 가장 높은 문자 프린트 (ypred)\n","- \"앞에서 사용된 9개 문자 + 새로 발생된 문자 1개\"를 input으로 사용 \n","- 반복을 통해 지정한 개수 만큼 문자 발생 \n","<img src=\"figures/text_gen_pred.png\" width=\"50%\">"]},{"cell_type":"markdown","metadata":{"id":"-x7Ktqe_aJyG","colab_type":"text"},"source":["- 문자열을 입력하여 그 다음에 나타날 확률일 가장 높은 문자를 무조건 발생시키면 비슷한 상황에서 언제나 같은 문자를 만들어냄\n","- 무조건 가장 높은 문자를 출력하는 대신 확률적으로 발생시킨다면 보다 다양한 문장을 생성할 수 있음 \n","    - e.g) {\"a\",\"b\",\"c\"}에 대한 output이 {0.2, 0.5, 0.3}인 경우 언제나 \"b\"를 출력하기 보다는 0.5의 확률로 \"b\"를 출력(multinomial distribution 활용)\n","- Output 값이 가장 큰 문자를 얼마나 \"확실히\" 출력할 것인가? temperature 값으로 조정하기 위해 `sample` 함수 사용"]},{"cell_type":"code","metadata":{"id":"B2bSIHU7aJyH","colab_type":"code","colab":{}},"source":["def sample(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    #print(preds.round(3))\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"sL3gWyvBaJyJ","colab_type":"code","outputId":"65ac723e-1d46-4fd2-d083-2895d3382264","colab":{}},"source":["sample([0.2, 0.5, 0.3], temperature=1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"gnE0xyHOaJyN","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","model=load_model('models/text_gen.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-10T09:12:39.962596Z","start_time":"2018-07-10T09:12:38.857949Z"},"id":"IKmpev8-aJyc","colab_type":"code","outputId":"1578ace2-3132-4c6b-e468-d2f6ce7edc8d","colab":{}},"source":["test_idx = np.random.randint(len(input_chars))\n","test_chars = input_chars[test_idx]\n","print(\"Generating from seed: %s\" % (test_chars))\n","print(test_chars, end=\"\")\n","for i in range(400):\n","    Xtest = np.zeros((1, SEQLEN, nb_chars))\n","    for i, ch in enumerate(test_chars):\n","        Xtest[0, i, char2index[ch]] = 1\n","    pred = model.predict(Xtest, verbose=0)[0]\n","    ypred = index2char[sample(pred, 0.5)]\n","    print(ypred, end=\"\")\n","    # move forward with test_chars + ypred\n","    test_chars = test_chars[1:] + ypred\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Generating from seed:  as usual.\n"," as usual.  `i dinmore the dacke"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["s the poor the harn, `and she parting the little work it your, and she was a better the mouted to the more pats to ofs were had fring in in the doran a lonfed ait the ding to on the in a little.  `i was she had to the paterny and asking the mouse of the queen sught the hersels and the cat, and chear bemand and she in thing the more said she began in cleng to the drears the hu"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8WP6Fpm6aJyh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}