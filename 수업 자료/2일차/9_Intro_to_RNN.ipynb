{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"9_Intro_to_RNN.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"a1xQI3EfZ3A8","colab_type":"text"},"source":["<font color=darkgreen>\n","    \n","# Lecture 9. Introduction to Recurrent Neural Network"]},{"cell_type":"markdown","metadata":{"id":"UMvo38bsZ3A_","colab_type":"text"},"source":["## 9.1 Example of sequence data\n","\n","#### Sequence-to-vector \n","\n","\n","- 예: Sentiment classification \n","    <img src=\"figures/sentiment.png\" width=\"50%\">\n","   \n","    - Input: 단어(또는 문자)를 순차적으로 입력\n","    - Output: 마지막 단어가 입력된 시점에 전체 문장의 감성을 분류\n","     <img src=\"figures/seq2one.PNG\" width=\"30%\">\n","\n","#### Sequence-to-sequence\n","- 예: Music generation \n","    <img src=\"figures/music.PNG\" width=\"50%\">\n","\n","    - Input: 20개의 순차적인 음정, 속도 등의 값 입력\n","    - Output: 이어지는 21번째 음정, 속도 값을 출력 \n","    <img src=\"figures/seq2seq.PNG\" width=\"30%\">\n","\n","\n","#### Delayed sequence-to-sequence\n","- 예: Machine translation\n","    <img src=\"figures/translation.PNG\" width=\"50%\">\n","\n","    - Input: 프랑스어 문장 전체를 단어 단위로 순차적 입력\n","    - Output: 번역된 영어 문장을 단어 단위로 순차적 출력\n","    <img src=\"figures/seq2seq_delay.PNG\" width=\"30%\">\n","\n","\n","#### Vector-to-sequence\n","- 예: Image captioning \n","    <img src=\"figures/imagecaption.png\" width=\"50%\">\n","\n","    - Input: 이미지\n","    - Output: 이미지에 대한 설명을 단어 단위로 순차적으로 출력 \n","    <img src=\"figures/one2seq.PNG\" width=\"30%\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cBHSmR4bZ3BA","colab_type":"text"},"source":["## 9.2 Recurrent neural networks\n","### Example\n","- 문장을 입력하여 각 단어가 사람이름을 나타내는지 여부를 구분 \n","- Tokenize\n","    - 문장을 모형의 입력단위인 단어(혹은 음절)로 쪼개는 작업\n","- Input: 단어\n","- Output: 1 if 사람이름, 0 elsewhere\n","\n","\n","<img src=\"figures/entity_exam.PNG\" width=\"50%\">\n","\n","- Vocabulary(dictionary) 구성 \n","    - 데이터 처리 시 사용할 모든 단어의 집합 (예, 10,000개의 단어 집합)\n","    - 각 단어를 $10000\\times 1$ 크기의 one-hot vector로 표현 \n","<img src=\"figures/vocabulary.PNG\" width=\"60%\">\n","<img src=\"figures/vocab2.png\" width=\"60%\">\n"]},{"cell_type":"markdown","metadata":{"id":"zr2yB4mZZ3BB","colab_type":"text"},"source":["### Recurrent neuron\n","- 현 시점의 input과 앞 시점의 hidden state를  동시에 input으로 받는 neuron\n","- 앞에 나온 단어가 현재 단어의 사람이름 여부에 영향을 미침\n","\n","<img src=\"figures/recurrent_neuron.PNG\" width=\"40%\">\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QDJe6vmkZ3BE","colab_type":"text"},"source":["$$ h_t = \\phi (x_t^T W_x + h_{t-1}^T W_h + b)$$"]},{"cell_type":"markdown","metadata":{"id":"IFHmbriSZ3BF","colab_type":"text"},"source":["### Recurrent neural networks\n","- Recurrent neuron을 여러 개로 하나의 layer를 구성 \n","<img src=\"figures/recurrent_layer.PNG\" width=\"50%\">\n","\n","- 여러 개의 layer를 쌓아서 stacked RNN을 구성 \n","<img src=\"figures/stacked_recurrent_layer.PNG\" width=\"50%\">\n"]},{"cell_type":"markdown","metadata":{"id":"B3j-ERA1Z3BF","colab_type":"text"},"source":["### Single RNN layer with Keras\n","- `input_shape=(timesteps, input_features)`\n","    - $10000\\times 1$의 one-hot vector로 표현되는 순차적인 3개 단어를 input \n","    - (Harry, Potter, and) -> 0\n","    - (Potter, and, Hermione) -> 1\n","- `return_sequences=False`\n","    - 마지막 시점의 output 만을 출력 \n","    - `(batch_size, output_features)`"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-09T16:14:16.917688Z","start_time":"2018-07-09T16:14:16.794825Z"},"scrolled":true,"id":"j8D-1froZ3BG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":434},"outputId":"b88c6f9d-1b27-429d-b6b9-0450e0b6f77b","executionInfo":{"status":"ok","timestamp":1575907486531,"user_tz":-540,"elapsed":2386,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["from keras.models import Sequential\n","from keras.layers import SimpleRNN, Dense\n","\n","model = Sequential()\n","model.add(SimpleRNN(32, input_shape=(3,10000), return_sequences=False))\n","model.add(Dense(1))\n","model.summary()\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","simple_rnn_1 (SimpleRNN)     (None, 32)                321056    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 321,089\n","Trainable params: 321,089\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xm7EmuxFZ3BL","colab_type":"text"},"source":["$$ h_t = \\phi (x_t^T W_x + h_{t-1}^T W_h + b)$$\n","- $W_x$: 10000개의 input 값과 32개 output($h_t$)과의 연결 = $32 \\times 10000 = 320000$\n","- $W_h$: 32개 $h_{t-1}$과 32개 $h_{t}$와의 연결 = $32 \\times 32 = 1024$ \n","- $b$: 32개 bias\n","\n","=> 320000 + 1024 + 32 = 321056"]},{"cell_type":"markdown","metadata":{"id":"Po-AhJC8Z3BM","colab_type":"text"},"source":["#### Stacked RNN layers with Keras\n","\n","- `return_sequences=True`\n","    - 매 시점의 output을 출력 \n","    - `(batch_size, timesteps, output_features)`\n","    - 다음 layer의 recurrent neuron에 입력하기 위해서는 매 시점의 output이 필요"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-09T16:14:29.089335Z","start_time":"2018-07-09T16:14:28.871189Z"},"id":"8WYef6vOZ3BM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":263},"outputId":"f4a5430c-c5d2-49b0-ef0a-16ac1697ad17","executionInfo":{"status":"ok","timestamp":1575907493410,"user_tz":-540,"elapsed":745,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["from keras.models import Sequential\n","from keras.layers import SimpleRNN, Dense\n","\n","model = Sequential()\n","model.add(SimpleRNN(32, input_shape=(3,10000), return_sequences=True))\n","model.add(SimpleRNN(32, input_shape=(3,10000), return_sequences=False))\n","model.add(Dense(1))\n","model.summary()\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","simple_rnn_2 (SimpleRNN)     (None, 3, 32)             321056    \n","_________________________________________________________________\n","simple_rnn_3 (SimpleRNN)     (None, 32)                2080      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 323,169\n","Trainable params: 323,169\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5jigQt9JZ3BP","colab_type":"text"},"source":["$$ h_t = \\phi (x_t^T W_x + h_{t-1}^T W_h + b)$$\n","- RNN layer 1\n","    - $W_x$: 10000개의 input 값과 32개 output($h_t$)과의 연결 = $32 \\times 10000 = 320000$\n","    - $W_h$: 32개 $h_{t-1}$과 32개 $h_{t}$와의 연결 = $32 \\times 32 = 1024$ \n","    - $b$: 32개 bias\n","    \n","    => 320000 + 1024 + 32 = 321056\n","- RNN layer 2\n","    - $W_x$: 32개의 input 값과 32개 output($h_t$)과의 연결 = $32 \\times 32 = 1024$\n","    - $W_h$: 32개 $h_{t-1}$과 32개 $h_{t}$와의 연결 = $32 \\times 32 = 1024$ \n","    - $b$: 32개 bias\n","    \n","    => 1024 + 1024 + 32 = 2080\n"]},{"cell_type":"markdown","metadata":{"id":"rckqdrwTZ3BQ","colab_type":"text"},"source":["## 9.3 The Problem of Long-Term Dependencies\n","- 이론적으로 RNN은 예전 입력값을 무한히 기억할 수 있음\n","- 학습과정에서 gradient가 불안정해져서 가중치의 학습이 잘 되지 못함\n","    - Vanishing gradient문제, exploding gradient문제\n","    - 처음의 input값이 점점 잊혀지는 현상 발생 \n","- ReLU activation, parameter initialization의 조정 등 보다 모형의 구조적으로 해결하려는 시도 \n","    - Long Short Term Memory(LSTM; Hochreiter & Schmidhuber, 1997)\n","    - Gated Recurrent Unit(GRU; Kyunghyun Cho et al., 2014) \n"]},{"cell_type":"markdown","metadata":{"id":"Xjod02juZ3BR","colab_type":"text"},"source":["## 9.3.1  LSTM cell\n","- 과거 입력값이 무조건 동일한 변환을 통해 전달되는 것이 아니라 필요에 따라 변환 없이 그대로 전달\n","    - f: forget gate, 이전 memory cell값을 얼마나 통과? \n","    - i: input gate, 새로운 정보를 얼마나 통과?\n","    - o: 현재 memory cell 값을 얼마나 외부 네트워크로 통과?\n","    - g: 현재 memory cell의 후보값 \n","    \n","<img src=\"figures/lstm.PNG\" width=\"40%\" align=\"left\">\n","<img src=\"figures/lstm2.png\" width=\"50%\">"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-09T17:44:28.587278Z","start_time":"2018-07-09T17:44:28.287507Z"},"scrolled":true,"id":"sbS5Z-56Z3BS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"8436df05-9702-4ac4-c77c-fea6383fc5b4","executionInfo":{"status":"ok","timestamp":1575907509895,"user_tz":-540,"elapsed":966,"user":{"displayName":"김동욱","photoUrl":"","userId":"05759155305116110709"}}},"source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","\n","model = Sequential()\n","model.add(LSTM(32, input_shape=(3,10000), return_sequences=False))\n","model.add(Dense(1))\n","model.summary()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_1 (LSTM)                (None, 32)                1284224   \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 1,284,257\n","Trainable params: 1,284,257\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wIDElDTOZ3BW","colab_type":"text"},"source":["## 9.3.2 GRU cell\n","- LSTM cell의 단순화되었지만 성능 좋음\n","- Cell state와 hidden state를 결합\n","- Input gate와 forget gate의 결합\n","\n","<img src=\"figures/gru.PNG\" width=\"40%\">"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-09T17:44:33.716457Z","start_time":"2018-07-09T17:44:33.564684Z"},"id":"3-3tdZMVZ3BX","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import GRU, Dense\n","\n","model = Sequential()\n","model.add(GRU(32, input_shape=(3,10000), return_sequences=False))\n","model.add(Dense(1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vNGixsASZ3BZ","colab_type":"text"},"source":["## 9.3.3 Bidirectional RNN cell\n","    He said, “Teddy bears are on sale!”\n","    He said, “Teddy Roosevelt was a great President!”\n","- \"Teddy\"가 사람이름을 나타내는지 여부는 앞보다 뒤의 단어들이 영향을 줌\n","- 시간 순서대로 적용한 RNN과 시간 역순대로 적용한 RNN을 합쳐서(concat, ave 등) 구성\n","<img src=\"figures/bidirectional_rnn.PNG\" width=\"30%\">"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-07-09T17:49:42.572652Z","start_time":"2018-07-09T17:49:41.438179Z"},"id":"BMfiVk85Z3Ba","colab_type":"code","colab":{},"outputId":"bd37c365-2c5e-4ad7-c724-fc681ea69757"},"source":["from keras.models import Sequential\n","from keras.layers import Bidirectional, LSTM, Dense\n","model = Sequential()\n","model.add(Bidirectional(LSTM(32, return_sequences=True),\n","                        input_shape=(3, 10000)))\n","model.add(Dense(1, activation='softmax'))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","bidirectional_4 (Bidirection (None, 3, 64)             2568448   \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 3, 1)              65        \n","=================================================================\n","Total params: 2,568,513\n","Trainable params: 2,568,513\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]}]}