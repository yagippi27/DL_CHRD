{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11. Word Embeddings and Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text data\n",
    "- Text data를 모형의 입력값으로 사용하기 위해서는 text를 숫자 vector로 표현하는 과정이 필요(vectorize)\n",
    "    - Text를 word 단위로 쪼개어 각 단어를 vector로 표현 \n",
    "    - Text를 character 단위로 쪼개어 각 character를 vector로 표현\n",
    "    - Text를 word나 character의 \"n-gram\"(연속된 n개의 word나 character)으로 쪼개여 각 \"n-gram\"을 vector로 표현 \n",
    "- Token: 텍스트를 쪼개어 만들어낸 입력 단위\n",
    "- One-hot encoding \n",
    "    - Text에 총 10000개의 단어가 있다면 각 단어를 0과 1로 이루어진 $10000 \\times 1$  vector로 표현 \n",
    "    - Input dimension의 수가 커지기 때문에 복잡도 증가 \n",
    "- Word embedding \n",
    "    - 저차원의 dense vector로 mapping하여 표현 \n",
    "    \n",
    "\n",
    "<img src=\"../figures/w2v.PNG\" width=\"40%\" align=\"left\">\n",
    "<img src=\"../figures/textvec.PNG\" width=\"30%\" >\n",
    "\n",
    " <br>   \n",
    "  <br> \n",
    "   <br> \n",
    "    \n",
    "<img src=\"https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png\" width=\"50%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "- 단어 사이의 추상적이고 기하학적인 관계를 반영하여 벡터화 하는 방법\n",
    "- 동의어가 비슷한 벡터로 임베딩이 될 수 있음(벡터 간의 거리가 가까움)\n",
    "- 단어 벡터 간의 방향이 의미를 내포할 수 있음 \n",
    "- 단어 벡터 간의 연산이 의미를 가짐\n",
    "    - man과 king의 관계 = woman과 queen의 관계 \n",
    "    - king - man + woman = queen\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*sXNXYfAqfLUeiDXPCo130w.png)\n",
    "\n",
    "![](https://blogs.mathworks.com/images/loren/2017/vecs.png)\n",
    "\n",
    "## Two approaches for word embedding\n",
    "1. Main task(document classification, sentiment classifiation 등)와 동시에 embedding 학습\n",
    "    - Prediction 성능에 최적화된 embedding 계산\n",
    "    - 단어 간의 의미상 거리가 잘 보존되지 않을 수 있음 \n",
    "    - Keras의 `Embedding` layer를 통해 학습 \n",
    "    \n",
    "2. Embedding을 따로 학습하고 사전학습된 embedding matrix를 사용하여 main task 수행 \n",
    "    - 단어 간의 의미상 거리를 보다 잘 표현하는 방법\n",
    "    - Prediction 성능은 저하될 수 있음 \n",
    "    - gensim의 Word2Vec을 통해 embedding을 학습한 후 keras input으로 활용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Learning word embeddings with the embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras embedding layer \n",
    "- Dictionary에 포함된 단어의 정수 index를 실수 벡터(dense vector)로 mapping하는 층\n",
    "- `Embedding` layer의 arguments\n",
    "    - `input_dim`: vocabulary의 크기. 즉, 총 vocabulary가 1000개의 단어를 포함하고 있으면 `input_dim=1000`\n",
    "    - `output_dim`: Dense vector로 표현되는 embedding vector의 크기(embedding dimensionality). 64차원의 벡터로 각 단어를 표현한다면 `output_dim=64`\n",
    "    - `input_length`: input data가 가지는 시퀀스의 길이. 즉, 몇 time step를 각 sample이 포함하는가? 몇 개의 단어로 하나의 입력문장이 이루어져 있는가?output_dim\n",
    "    \n",
    "\n",
    "- `Embedding` layer에 입력되는 데이터의 shape = `(samples, input_length)`\n",
    "    - 단어의 index를 포함하는 array\n",
    "    - 단어 10개로 이루어진 문장이 32개 포함된 batch일 경우 (32, 10) \n",
    "    - 모든 sequence는 같은 길이어야 함. (같은 사이즈의 np.array를 입력받음)\n",
    "        - 만일 길이가 짧다면 0을 넣어 길이를 동일하게 맞추어 줌 \n",
    "\n",
    "\n",
    "- `Embedding` layer의  Output shape =  `(samples, input_length, output_dim)` \n",
    "    - RNN의 input으로 사용 가능 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:02:19.922986Z",
     "start_time": "2018-07-10T17:02:19.917314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 64)            64000     \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=20))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $(1000 \\times 1)$ one-hot vector를 $(64 \\times 1)$ dense vector로 변환\n",
    "- 단어 20개로 이루어진 문장을 단어 index로 표현하여 입력하면 각 단어가 $(64 \\times 1)$ dense vector로 표현된 결과로 출력됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sentiment analysis for movie review\n",
    "- 영화리뷰 문장이 긍정/부정인지 판단하는 모형 \n",
    "- Input = “The Da Vinci Code book is just awesome.”\n",
    "- Output = 1 (긍정)\n",
    "- 한 문장의 각 단어를 매 시점의 input으로 사용 \n",
    "- 매 단어(시점)마다 긍/부정의 답이 필요하지 않으므로 마지막 시점에 대한  output만을 사용: many-to-one 구조\n",
    "<img src=\"../figures/sentiment.png\" width=\"40%\">\n",
    "(umich-sentiment-train.txt)\n",
    "\n",
    "        1\tThe Da Vinci Code book is just awesome.\n",
    "        1\tthis was the first clive cussler i've ever read, but even books like Relic, and Da Vinci code were more plausible than this.\n",
    "        1\ti liked the Da Vinci Code a lot.\n",
    "        1\ti liked the Da Vinci Code a lot.\n",
    "        1\tI liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.\n",
    "        1\tthat's not even an exaggeration ) and at midnight we went to Wal-Mart to buy the Da Vinci Code, which is amazing of course.\n",
    "        1\tI loved the Da Vinci Code, but now I want something better and different!..\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000   \n",
    "MAX_SENTENCE_LENGTH = 40  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `MAX_FEATURES = 2000`: 최대 2000개의 단어를 사용하여 dictionary 구성  \n",
    "- `MAX_SENTENCE_LENGTH = 40`: 문장의 최대 길이을 40개 단어로 제한 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SW교육03\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"../data/umich-sentiment-train.txt\", 'rb')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\") # split(\"\"\\t)공백 마다 짜르기.\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)  # the maximum number of words in a sentence\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1  # frequency for each word\n",
    "    num_recs += 1 # total number of records\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- collections.Counter: dict의 subclass로 요소들의 개수 저장\n",
    "- 단어의 출현 빈도를 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 2326)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen, len(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 2328개의 단어가 포함되어 있음\n",
    "- 가장 긴 문장이 42개의 단어로 구성됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "##### --- Practice line-by-line ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"../data/umich-sentiment-train.txt\", 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = b'1\\tThe Da Vinci Code book is just awesome.\\n'\n",
    "label, sentence = line.decode('utf8').strip().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the da vinci code book is just awesome.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    word_freqs[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 1,\n",
       "         'da': 1,\n",
       "         'vinci': 1,\n",
       "         'code': 1,\n",
       "         'book': 1,\n",
       "         'is': 1,\n",
       "         'just': 1,\n",
       "         'awesome': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"../data/umich-sentiment-train.txt\", 'rb')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)  # the maximum number of words in a sentence\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1  # frequency for each word\n",
    "    num_recs += 1 # total number of records\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 is UNK, 0 is PAD\n",
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0   # for sentences shorter than MAX_SENTENCE_LENGTH\n",
    "word2index[\"UNK\"] = 1   # for words not in the vocabulary\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `word2index`: 단어를 index로 mapping하는 dictionary\n",
    "- `index2word`: index를 단어로 mapping하는 dictionary\n",
    "- `\"PAD\"`: 입력 데이터의 차원을 동일하게 만들기 위해 문장의 길이가 MAX_SENTENCE_LENGTH보다 짧을 경우 `\"PAD\"`를 채워줌 (index=0)\n",
    "- `\"UNK\"`: dictionary에 포함되지 않은 단어는 `\"UNK\"`으로 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"../data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 9, 12, 101, 17, 48, 22, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   5,  10,   9,  12, 101,  17,  48,  22,\n",
       "         4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장을 단어의 index로 변환하여 list로 생성\n",
    "- 문장의 길이가 `MAX_SENTENCE_LENGTH`보다 작으면 0으로 padding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 40) (1418, 40) (5668,) (1418,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 128)           256256    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 276,897\n",
      "Trainable params: 276,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_SENTENCE_LENGTH, mask_zero = True))\n",
    "model.add(LSTM(32, recurrent_dropout=0.2, return_sequences = False, activation = 'relu'))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vocab_size` 크기의 dictionary로 표현된 input을 EMBEDDING_SIZE로 표현 \n",
    "- `input_length=MAX_SENTENCE_LENGTH`: 뒤이어 연결되는 recurrent layer에 입력을 위한 순차적인 입력단어의 수 \n",
    "- `mask_zero = True`: 0으로 padding 한 부분을 학습시 사용하지 않음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/20\n",
      "5668/5668 [==============================] - 9s 2ms/step - loss: 0.0091 - acc: 0.9988 - val_loss: 0.0328 - val_acc: 0.9915\n",
      "Epoch 2/20\n",
      "5668/5668 [==============================] - 2s 282us/step - loss: 0.0155 - acc: 0.9993 - val_loss: 0.0321 - val_acc: 0.9922\n",
      "Epoch 3/20\n",
      "5668/5668 [==============================] - 1s 263us/step - loss: 0.0182 - acc: 0.9986 - val_loss: 0.0312 - val_acc: 0.9922\n",
      "Epoch 4/20\n",
      "5668/5668 [==============================] - 2s 274us/step - loss: 0.0154 - acc: 0.9986 - val_loss: 0.0410 - val_acc: 0.9873\n",
      "Epoch 5/20\n",
      "5668/5668 [==============================] - 3s 487us/step - loss: 0.0228 - acc: 0.9975 - val_loss: 0.0377 - val_acc: 0.9880\n",
      "Epoch 6/20\n",
      "5668/5668 [==============================] - 3s 608us/step - loss: 0.0254 - acc: 0.9984 - val_loss: 0.0342 - val_acc: 0.9908\n",
      "Epoch 7/20\n",
      "5668/5668 [==============================] - 2s 397us/step - loss: 0.0209 - acc: 0.9986 - val_loss: 0.0327 - val_acc: 0.9915\n",
      "Epoch 8/20\n",
      "5668/5668 [==============================] - 2s 342us/step - loss: 0.0147 - acc: 0.9991 - val_loss: 0.0321 - val_acc: 0.9929\n",
      "Epoch 9/20\n",
      "5668/5668 [==============================] - 2s 315us/step - loss: 0.0120 - acc: 0.9993 - val_loss: 0.0395 - val_acc: 0.9901\n",
      "Epoch 10/20\n",
      "5668/5668 [==============================] - 2s 298us/step - loss: 0.0191 - acc: 0.9989 - val_loss: 0.0381 - val_acc: 0.9915\n",
      "Epoch 11/20\n",
      "5668/5668 [==============================] - 2s 283us/step - loss: 0.0145 - acc: 0.9991 - val_loss: 0.0349 - val_acc: 0.9929\n",
      "Epoch 12/20\n",
      "5668/5668 [==============================] - 2s 274us/step - loss: 0.0143 - acc: 0.9995 - val_loss: 0.0327 - val_acc: 0.9929\n",
      "Epoch 13/20\n",
      "5668/5668 [==============================] - 5s 964us/step - loss: 0.0155 - acc: 0.9993 - val_loss: 0.0317 - val_acc: 0.9915\n",
      "Epoch 14/20\n",
      "5668/5668 [==============================] - 5s 811us/step - loss: 0.0094 - acc: 0.9993 - val_loss: 0.0321 - val_acc: 0.9922\n",
      "Epoch 15/20\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0054 - acc: 0.9998 - val_loss: 0.0338 - val_acc: 0.9922\n",
      "Epoch 16/20\n",
      "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0106 - acc: 0.9995 - val_loss: 0.0363 - val_acc: 0.9922\n",
      "Epoch 17/20\n",
      "5668/5668 [==============================] - 5s 802us/step - loss: 0.0104 - acc: 0.9995 - val_loss: 0.0386 - val_acc: 0.9922\n",
      "Epoch 18/20\n",
      "5668/5668 [==============================] - 3s 581us/step - loss: 0.0102 - acc: 0.9995 - val_loss: 0.0406 - val_acc: 0.9922\n",
      "Epoch 19/20\n",
      "5668/5668 [==============================] - 5s 959us/step - loss: 0.0099 - acc: 0.9995 - val_loss: 0.0410 - val_acc: 0.9922\n",
      "Epoch 20/20\n",
      "5668/5668 [==============================] - 4s 629us/step - loss: 0.0097 - acc: 0.9995 - val_loss: 0.0410 - val_acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa7fe57988>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='models/sentiment_analysis.h5', monitor='val_loss', save_best_only=True),\n",
    "    TensorBoard(log_dir='logs/sentiment_analysis/'+now),\n",
    "    EarlyStopping(monitor='val_loss',patience=3)\n",
    "]\n",
    "# model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(Xtest, ytest), callbacks=callbacks_list)\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=20, validation_data=(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'models/sentiment_analysis.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-72d0f1c12eb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/sentiment_analysis.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\keras\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\keras\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\keras\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'models/sentiment_analysis.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model=load_model('models/sentiment_analysis.h5')\n",
    "model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TSNE' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d2867215686a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mylabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%.0f\\t%d\\t%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TSNE' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(Xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 11.2 Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 풀고자 하는 main task와 함께 word embedding을 학습하는 대신 미리 계산된 embedding 공간에서 vector를 로드하여 사용\n",
    "- 충분한 데이터로 학습한 embedding vector는 언어구조의 일반적인 측면을 잡아낼 수 있음\n",
    "- Word embedding을 계산하는 알고리즘\n",
    "    - Word2vec(skip-gram, CBow)\n",
    "    - GloVe\n",
    "    - Fasttext\n",
    "    - ELMo\n",
    "    - BERT\n",
    "    - KoBERT(ETRI에서 제작한 한국어 알고리즘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec (Mikolov in Google, 2013)\n",
    "- Skipgram: 문장에서 중심 단어들로 주변에 위치한 단어들의 출현을 예측하는 알고리즘\n",
    "<img src=\"http://i.imgur.com/TupGxMl.png\" width=\"50%\" align=\"left\" >\n",
    "<img src=\"http://i.imgur.com/8zNRwsn.png\" width=\"50%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec으로 학습 된 embedding matrix를 keras의 embedding layer에 넣어 prediction model 학습\n",
    "- Input: 단어들로 이루어진 list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toy example\n",
    "input_text = [['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.'],\n",
    "              ['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/09/ed/b59a2edde05b7f5755ea68648487c150c7c742361e9c8733c6d4ca005020/gensim-3.8.1-cp37-cp37m-win_amd64.whl (24.2MB)\n",
      "Requirement already satisfied: six>=1.5.0 in d:\\anaconda\\envs\\keras\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\envs\\keras\\lib\\site-packages (from gensim) (1.3.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda\\envs\\keras\\lib\\site-packages (from gensim) (1.17.4)\n",
      "Collecting boto>=2.32\n",
      "  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/be/04/7744980154abb220792503968cf0aac28a19f5c9be974635c747b553adf1/boto3-1.10.36-py2.py3-none-any.whl (128kB)\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\keras\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.14.0,>=1.13.36\n",
      "  Downloading https://files.pythonhosted.org/packages/08/e0/e47a793322fd68381e4cda4f98740c558c9ef2a87e71d06589048f59cbcb/botocore-1.13.36-py2.py3-none-any.whl (5.8MB)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "Collecting python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-1.9.0-cp37-none-any.whl size=73092 sha256=b80276fcdf66e76bd7bf5d87fa07a8fa1a65f0527e1a225f23f849581e9f6f51\n",
      "  Stored in directory: C:\\Users\\SW교육03\\AppData\\Local\\pip\\Cache\\wheels\\ab\\10\\93\\5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, idna, urllib3, chardet, requests, jmespath, python-dateutil, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "Successfully installed boto-2.49.0 boto3-1.10.36 botocore-1.13.36 chardet-3.0.4 docutils-0.15.2 gensim-3.8.1 idna-2.8 jmespath-0.9.4 python-dateutil-2.8.0 requests-2.22.0 s3transfer-0.2.1 smart-open-1.9.0 urllib3-1.25.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "embed_model = Word2Vec(input_text, sg=1, size=5, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sg=1`: skipgram 알고리즘 사용 \n",
    "- `size=5`: embedding size \n",
    "- `window=5`: 주변의 5개 단어를 사용하여 해당 단어의 출현 예측 \n",
    "- `min_count=1`: 최소 1번 이상 출현한 단어들만 사용 (default=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:03:10.031128Z",
     "start_time": "2018-07-10T17:03:09.018550Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [v for v, _ in word_freqs.most_common(MAX_FEATURES)]\n",
    "\n",
    "sentences = np.empty((num_recs, ), dtype=list)\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            sentence.append(word)\n",
    "        else:\n",
    "            sentence.append(\"UNK\")\n",
    "    sentences[i] = sentence\n",
    "    i += 1\n",
    "    \n",
    "ftrain.close()\n",
    "\n",
    "sentences=list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7086"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장을 단어로 쪼개어 단어로 이루어진  list로 변환 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec by gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:04:11.187511Z",
     "start_time": "2018-07-10T17:04:11.183966Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embed_model = Word2Vec(sentences, sg=1, size=EMBEDDING_SIZE, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " ',',\n",
       " '.',\n",
       " 'the',\n",
       " 'and',\n",
       " '!',\n",
       " 'harry',\n",
       " 'vinci',\n",
       " 'da',\n",
       " 'brokeback',\n",
       " 'code',\n",
       " 'mountain',\n",
       " 'potter',\n",
       " '...',\n",
       " 'love',\n",
       " 'is',\n",
       " 'a',\n",
       " 'was',\n",
       " 'mission',\n",
       " 'impossible',\n",
       " 'awesome',\n",
       " 'like',\n",
       " 'it',\n",
       " 'to',\n",
       " 'movie',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'because',\n",
       " 'sucks',\n",
       " 'hate',\n",
       " 'sucked',\n",
       " 'so',\n",
       " 'as',\n",
       " 'my',\n",
       " '``',\n",
       " 'much',\n",
       " 'of',\n",
       " 'really',\n",
       " 'movies',\n",
       " 'stupid',\n",
       " 'you',\n",
       " 'UNK',\n",
       " 'down',\n",
       " 'but',\n",
       " 'we',\n",
       " 'be',\n",
       " 'with',\n",
       " 'just',\n",
       " 'one',\n",
       " 'potter..',\n",
       " 'know',\n",
       " 'suck',\n",
       " 'out',\n",
       " '3',\n",
       " 'or',\n",
       " 'who',\n",
       " '/',\n",
       " 'am',\n",
       " 'loved',\n",
       " 'want',\n",
       " 'into',\n",
       " 'which',\n",
       " 'right',\n",
       " 'for',\n",
       " 'an',\n",
       " \"n't\",\n",
       " 'this',\n",
       " 'me',\n",
       " ':',\n",
       " 'are',\n",
       " 'think',\n",
       " 'not',\n",
       " 'how',\n",
       " 'if',\n",
       " 'depressing',\n",
       " 'people',\n",
       " 'his',\n",
       " 'would',\n",
       " 'up',\n",
       " 'reading',\n",
       " 'why',\n",
       " 'series',\n",
       " 'there',\n",
       " 'only',\n",
       " 'in',\n",
       " 'fucking',\n",
       " 'terrible',\n",
       " 'she',\n",
       " 'story',\n",
       " 'oh',\n",
       " 'being',\n",
       " '..',\n",
       " 'left',\n",
       " 'guy',\n",
       " 'here',\n",
       " 'ok',\n",
       " 'start',\n",
       " 'felicia',\n",
       " '(',\n",
       " 'have',\n",
       " 'book',\n",
       " 'about',\n",
       " 'beautiful',\n",
       " 'good',\n",
       " 'also',\n",
       " 'too',\n",
       " 'on',\n",
       " 'do',\n",
       " 'went',\n",
       " 'read',\n",
       " 'at',\n",
       " 'then',\n",
       " 'can',\n",
       " 'saw',\n",
       " 'first',\n",
       " 'by',\n",
       " 'tom',\n",
       " 'more',\n",
       " 'thought',\n",
       " '2',\n",
       " 'most',\n",
       " 'from',\n",
       " 'way',\n",
       " 'liked',\n",
       " 'absolutely',\n",
       " 'still',\n",
       " 'horrible',\n",
       " 'well',\n",
       " 'time',\n",
       " 'got',\n",
       " 'he',\n",
       " 'when',\n",
       " 'awesome..',\n",
       " 'big',\n",
       " 'heard',\n",
       " 'were',\n",
       " 'ever',\n",
       " 'watch',\n",
       " 'what',\n",
       " 'film',\n",
       " 'going',\n",
       " 'great',\n",
       " 'things',\n",
       " 'such',\n",
       " 'better',\n",
       " 'does',\n",
       " 'said',\n",
       " 'last',\n",
       " 'ca',\n",
       " '?',\n",
       " '=',\n",
       " 'gay',\n",
       " 'yeah',\n",
       " 'boring',\n",
       " 'watching',\n",
       " 'both',\n",
       " 'never',\n",
       " 'making',\n",
       " 'wait',\n",
       " 'again',\n",
       " 'man',\n",
       " 'these',\n",
       " 'same',\n",
       " '’',\n",
       " \"'re\",\n",
       " 'person',\n",
       " 'friends',\n",
       " 'excellent',\n",
       " 'always',\n",
       " 'sucked..',\n",
       " 'friday',\n",
       " 'anyone',\n",
       " 'cock',\n",
       " 'opinion',\n",
       " 'na',\n",
       " 'says',\n",
       " 'cool',\n",
       " 'make',\n",
       " 'anyway',\n",
       " 'him',\n",
       " 'rocks',\n",
       " 'take',\n",
       " 'side',\n",
       " 'knows',\n",
       " 's',\n",
       " 'worth',\n",
       " 'where',\n",
       " 'around',\n",
       " 'fun',\n",
       " 'mom',\n",
       " 'either',\n",
       " 'community',\n",
       " 'wanted',\n",
       " 'dad',\n",
       " 'luv',\n",
       " 'hates',\n",
       " 'main',\n",
       " 'thats',\n",
       " 'review',\n",
       " 'b',\n",
       " 'care',\n",
       " 'freakin',\n",
       " 'character',\n",
       " 'place',\n",
       " 'daniel',\n",
       " 'begin',\n",
       " 'crazy',\n",
       " 'awards',\n",
       " 'soo',\n",
       " 'turned',\n",
       " 'head',\n",
       " 'hat',\n",
       " 'table',\n",
       " 'cowboy',\n",
       " 'reality',\n",
       " 'acceptable',\n",
       " 'snuck',\n",
       " 'seen..',\n",
       " 'gon',\n",
       " 'becoming',\n",
       " 'horrible..',\n",
       " 'type',\n",
       " 'past',\n",
       " 'dies',\n",
       " 'bitch',\n",
       " 'stand',\n",
       " 'needs',\n",
       " 'deep',\n",
       " \"'yeah\",\n",
       " 'serious',\n",
       " 'slap',\n",
       " 'zen',\n",
       " 'hill',\n",
       " 'panting',\n",
       " 'kelsie',\n",
       " 'laughed',\n",
       " 'catcher',\n",
       " 'jokes..',\n",
       " 'malfoy',\n",
       " 'outshines',\n",
       " 'blonds',\n",
       " 'leah',\n",
       " 'profound',\n",
       " 'black',\n",
       " 'dudeee',\n",
       " 'combining',\n",
       " 'draco',\n",
       " 'gary',\n",
       " 'wotshisface',\n",
       " 'likes',\n",
       " 'quiz',\n",
       " 'noises',\n",
       " 'escapades',\n",
       " 'dash',\n",
       " 'virgin',\n",
       " 'trousers',\n",
       " 'material',\n",
       " \"'..\",\n",
       " 'joining',\n",
       " 'hoot',\n",
       " 'vigor',\n",
       " 'letting',\n",
       " 'kirsten',\n",
       " 'goin',\n",
       " 'tye',\n",
       " 'differently',\n",
       " 'cleaning',\n",
       " 'stars',\n",
       " 'keys',\n",
       " 'throat',\n",
       " 'lubb',\n",
       " 'retarted',\n",
       " 'rock-hard',\n",
       " 'homosexuality',\n",
       " 'gin',\n",
       " 'coz',\n",
       " 'reminded',\n",
       " 'silent',\n",
       " 'kate',\n",
       " 'helped',\n",
       " 'desperately',\n",
       " 'mtv',\n",
       " 'plain',\n",
       " \"love'the\",\n",
       " 'aching',\n",
       " 'count',\n",
       " 'hung',\n",
       " 'bobbypin',\n",
       " 'insanely',\n",
       " 'dragged',\n",
       " 'despised',\n",
       " '2.5',\n",
       " 'station',\n",
       " 'suicides',\n",
       " 'bonkers',\n",
       " 'sentry',\n",
       " 'hips',\n",
       " 'whimpering',\n",
       " 'grabs',\n",
       " 'hella',\n",
       " 'acne',\n",
       " 'sit',\n",
       " 'eyre',\n",
       " 'groaning',\n",
       " 'jane',\n",
       " 'bye..',\n",
       " 'see',\n",
       " ')',\n",
       " 'iii',\n",
       " 'hated',\n",
       " 'all',\n",
       " \"'m\",\n",
       " 'did',\n",
       " 'pretty',\n",
       " 'though',\n",
       " 'evil',\n",
       " 'even',\n",
       " 'ass',\n",
       " 'books',\n",
       " 'say',\n",
       " 'they',\n",
       " 'go',\n",
       " 'had',\n",
       " '*',\n",
       " 'now',\n",
       " 'lot',\n",
       " 'get',\n",
       " 'other',\n",
       " 'cruise',\n",
       " 'will',\n",
       " 'after',\n",
       " 'very',\n",
       " '--',\n",
       " 'some',\n",
       " '<',\n",
       " 'watched',\n",
       " 'its',\n",
       " 'best',\n",
       " 'kinda',\n",
       " 'amazing',\n",
       " 'balls',\n",
       " 'actually',\n",
       " 'than',\n",
       " 'no',\n",
       " 'seen',\n",
       " 'already',\n",
       " 'awful',\n",
       " 'made',\n",
       " 'lol',\n",
       " 'mountain..',\n",
       " 'may',\n",
       " 'two',\n",
       " 'second',\n",
       " 'has',\n",
       " 'since',\n",
       " 'thing',\n",
       " 'sad',\n",
       " \"'ve\",\n",
       " 'off',\n",
       " 'new',\n",
       " 'us',\n",
       " \"'d\",\n",
       " 'over',\n",
       " 'been',\n",
       " 'miss',\n",
       " \"'ll\",\n",
       " 'far',\n",
       " ']',\n",
       " '[',\n",
       " 'your',\n",
       " 'action',\n",
       " '&',\n",
       " 'tell',\n",
       " 'real',\n",
       " 'crash',\n",
       " 'theme',\n",
       " 'should',\n",
       " 'shit',\n",
       " 'talk',\n",
       " 'enjoy',\n",
       " 'three',\n",
       " 'those',\n",
       " 'night',\n",
       " 'could',\n",
       " 'everyone',\n",
       " 'before',\n",
       " 'looks',\n",
       " 'code..',\n",
       " 'having',\n",
       " 'them',\n",
       " 'while',\n",
       " 'enjoyed',\n",
       " 'bad',\n",
       " '>',\n",
       " 'life',\n",
       " 'sucks..',\n",
       " 'yet',\n",
       " 'try',\n",
       " 'feel',\n",
       " 'mean',\n",
       " 'thinking',\n",
       " 'little',\n",
       " 'something',\n",
       " 'anything',\n",
       " 'talking',\n",
       " 'else',\n",
       " \"'\",\n",
       " 'times',\n",
       " 'wrong',\n",
       " 'world',\n",
       " 'hey',\n",
       " 'course',\n",
       " 'interesting',\n",
       " 'totally',\n",
       " 'glad',\n",
       " 'sure',\n",
       " 'x-men',\n",
       " 'might',\n",
       " 'back',\n",
       " 'crappy',\n",
       " 'sucking',\n",
       " 'okay',\n",
       " 'demons',\n",
       " 'movie..',\n",
       " 'told',\n",
       " 'finished',\n",
       " 'seeing',\n",
       " 'quite',\n",
       " 'long',\n",
       " 'much..',\n",
       " 'kids',\n",
       " 'fan',\n",
       " 'lost',\n",
       " 'bit',\n",
       " 'school',\n",
       " 'freaking',\n",
       " 'fandom',\n",
       " 'every',\n",
       " '-',\n",
       " 'year',\n",
       " 'saying',\n",
       " 'almost',\n",
       " 'god',\n",
       " 'picture',\n",
       " 'cowboys',\n",
       " 'our',\n",
       " 'day',\n",
       " 'fact',\n",
       " 'give',\n",
       " 'doing',\n",
       " 'personally',\n",
       " 'money',\n",
       " 'end',\n",
       " 'mother',\n",
       " 'few',\n",
       " 'until',\n",
       " 'theater',\n",
       " 'novel',\n",
       " 'least',\n",
       " 'tonight',\n",
       " 'angels',\n",
       " 'im',\n",
       " 'inaccurate',\n",
       " 'sick',\n",
       " 'probably',\n",
       " 'their',\n",
       " 'apparently',\n",
       " 'hear',\n",
       " 'stories',\n",
       " 'crap',\n",
       " 'officially',\n",
       " 'won',\n",
       " 'lord',\n",
       " 'must',\n",
       " 'culture',\n",
       " 'used',\n",
       " 'luck',\n",
       " '”',\n",
       " 'news',\n",
       " 'rings',\n",
       " 'bogus',\n",
       " 'show',\n",
       " 'through',\n",
       " 'kind',\n",
       " '$',\n",
       " 'classes',\n",
       " 'girl',\n",
       " 'omg',\n",
       " 'hollywood',\n",
       " 'children',\n",
       " 'oscar',\n",
       " 'fire',\n",
       " 'someone',\n",
       " 'once',\n",
       " 'different',\n",
       " 'john',\n",
       " 'her',\n",
       " 'played',\n",
       " 'films',\n",
       " 'club',\n",
       " 'own',\n",
       " 'come',\n",
       " 'georgia',\n",
       " 'damn',\n",
       " 'woo',\n",
       " 'hope',\n",
       " 'major',\n",
       " 'nearly',\n",
       " 'move',\n",
       " 'jesus',\n",
       " 'ass..',\n",
       " 'shitty',\n",
       " 'telling',\n",
       " 'fuck',\n",
       " 'board',\n",
       " 'song',\n",
       " 'too..',\n",
       " 'days',\n",
       " 'fell',\n",
       " 'yes',\n",
       " 'wan',\n",
       " 'sexy',\n",
       " 'idea',\n",
       " 'part',\n",
       " 'majorly',\n",
       " 'comes',\n",
       " 'finally',\n",
       " 'decided',\n",
       " 'half',\n",
       " 'incredibly',\n",
       " 'music',\n",
       " 'week',\n",
       " 'funny',\n",
       " '3..',\n",
       " 'ta',\n",
       " 'playing',\n",
       " 'better..-we',\n",
       " 'racism',\n",
       " 'next',\n",
       " 'fat',\n",
       " 'update',\n",
       " 'agree',\n",
       " 'during',\n",
       " 'gun',\n",
       " 'let',\n",
       " '_',\n",
       " 'goblet',\n",
       " 'looking',\n",
       " 'evil..',\n",
       " 'cried',\n",
       " 'felt',\n",
       " 'whole',\n",
       " 'especially',\n",
       " '“',\n",
       " 'hard',\n",
       " 'thank',\n",
       " 'dont',\n",
       " '6th',\n",
       " 'eragon',\n",
       " '�',\n",
       " 'win',\n",
       " 'pages',\n",
       " 'stinks',\n",
       " ';',\n",
       " 'hands',\n",
       " 'heart',\n",
       " 'cuz',\n",
       " 'absurd',\n",
       " 'given',\n",
       " 'lit',\n",
       " 't',\n",
       " 'fanfiction',\n",
       " 'college',\n",
       " 'work',\n",
       " 'rant',\n",
       " 'top',\n",
       " 'nothing',\n",
       " 'example',\n",
       " 'conclusion',\n",
       " 'rather',\n",
       " 'gorgeous',\n",
       " 'erm',\n",
       " 'ask',\n",
       " 'taking',\n",
       " 'sort',\n",
       " 'hedge',\n",
       " 'jake',\n",
       " 'bullshit',\n",
       " 'death',\n",
       " 'lame',\n",
       " 'dan',\n",
       " 'minutes',\n",
       " 'stuff',\n",
       " 'many',\n",
       " 'sorry',\n",
       " 'except',\n",
       " 'nc-17',\n",
       " 'disliked',\n",
       " 'besides',\n",
       " 'halloween',\n",
       " 'wondering',\n",
       " 'jack',\n",
       " 'brown',\n",
       " 'wish',\n",
       " 'loves',\n",
       " 'm',\n",
       " 'extremely',\n",
       " 'hell',\n",
       " 'stop',\n",
       " 'books..',\n",
       " 'between',\n",
       " 'ban',\n",
       " '4',\n",
       " 'level',\n",
       " 'gift',\n",
       " 'anyways',\n",
       " 'food',\n",
       " 'short',\n",
       " 'mouth',\n",
       " 'donkey',\n",
       " 'case',\n",
       " 'ten',\n",
       " 'came',\n",
       " 'true',\n",
       " 'myself',\n",
       " 'rent',\n",
       " 'passion',\n",
       " 'looked',\n",
       " 'any',\n",
       " 'god-awful',\n",
       " 'none',\n",
       " 'write',\n",
       " 'suck..',\n",
       " 'comment',\n",
       " 'worthless',\n",
       " 'critics',\n",
       " '+',\n",
       " 'need',\n",
       " 'kick',\n",
       " 'audrey',\n",
       " 'religious',\n",
       " 'stayed',\n",
       " 'tautou',\n",
       " 'live',\n",
       " 'shows',\n",
       " 'along',\n",
       " 'deal',\n",
       " 'rowling',\n",
       " 'score',\n",
       " 'blame',\n",
       " 'nerd',\n",
       " 'sooo',\n",
       " 'chinese',\n",
       " 'whistles',\n",
       " 'wesley',\n",
       " 'recently',\n",
       " 'dance',\n",
       " 'together',\n",
       " 'tc',\n",
       " 'religion',\n",
       " '\\x80',\n",
       " 'till',\n",
       " 'it..',\n",
       " 'awesomest',\n",
       " 'dumb',\n",
       " 'wo',\n",
       " 'dragons',\n",
       " 'figures',\n",
       " 'soon',\n",
       " 'rented',\n",
       " 'accompaniment',\n",
       " 'robbed',\n",
       " 'hoffman',\n",
       " 'weekend',\n",
       " 'ron',\n",
       " 'kiss',\n",
       " 'coming',\n",
       " 'cocktail',\n",
       " 'decent',\n",
       " 'theaters',\n",
       " 'costume',\n",
       " 'twist',\n",
       " 'super',\n",
       " 'whenever',\n",
       " 'often',\n",
       " 'franchise',\n",
       " 'equal',\n",
       " 'nice',\n",
       " 'cut',\n",
       " 'home',\n",
       " 'kid',\n",
       " 'cute',\n",
       " 'fireworks',\n",
       " 'exquisite',\n",
       " 'code-sucked',\n",
       " 'hide',\n",
       " 'speaking',\n",
       " 'events',\n",
       " 'education',\n",
       " 'ruined',\n",
       " 'working',\n",
       " 'libraries',\n",
       " 'tried',\n",
       " 'butt',\n",
       " '7',\n",
       " 'emma',\n",
       " 'ticket',\n",
       " 'ang',\n",
       " 'hardcore',\n",
       " 'knew',\n",
       " 'beautiful..',\n",
       " 'simply',\n",
       " 'mirror',\n",
       " 'forgotten',\n",
       " 'dislike',\n",
       " 'requested',\n",
       " 'bible',\n",
       " 'empty',\n",
       " 'phillip',\n",
       " 'aka',\n",
       " 'definitely',\n",
       " 'reply',\n",
       " 'craze',\n",
       " 'fanfic',\n",
       " '–',\n",
       " 'able',\n",
       " 'issues',\n",
       " 'shittiest',\n",
       " 'obnoxious',\n",
       " 'la',\n",
       " 'scar',\n",
       " 'marcia',\n",
       " 'feeling',\n",
       " 'flick',\n",
       " 'book..',\n",
       " 'respect',\n",
       " 'hero',\n",
       " 'dream',\n",
       " 'due',\n",
       " 'tho',\n",
       " 'hold',\n",
       " 'run',\n",
       " 'murderball-immediately',\n",
       " 'loathe',\n",
       " 'ya',\n",
       " 'goth',\n",
       " 'maybe',\n",
       " 'attempt',\n",
       " \"y'all\",\n",
       " 'page',\n",
       " 'chris',\n",
       " 'whether',\n",
       " 'third',\n",
       " 'white',\n",
       " 'friend',\n",
       " '\\x99',\n",
       " 'carefully',\n",
       " 'unbelievably',\n",
       " 'aaron',\n",
       " 'thousand',\n",
       " 'saturday',\n",
       " 'idk',\n",
       " 'pop',\n",
       " 'academy',\n",
       " 'sometimes',\n",
       " 'cringe',\n",
       " 'hot',\n",
       " 'happy',\n",
       " 'color',\n",
       " 'months',\n",
       " 'royally',\n",
       " 'queer',\n",
       " 'stone',\n",
       " 'guys',\n",
       " 'challenge',\n",
       " 'boring..',\n",
       " '-and',\n",
       " 'rest',\n",
       " 'captain',\n",
       " 'suppose',\n",
       " 'point',\n",
       " 'record',\n",
       " 'sivullinen',\n",
       " 'against',\n",
       " 'laid',\n",
       " 'beat',\n",
       " 'k.',\n",
       " 'meeting',\n",
       " 'enough',\n",
       " 'special',\n",
       " 'adorable',\n",
       " 'mi3',\n",
       " 'deserved',\n",
       " 'everybody',\n",
       " 'johnny',\n",
       " '-we',\n",
       " 'joke',\n",
       " 'final',\n",
       " 'less',\n",
       " 'christmas',\n",
       " 'dementors',\n",
       " 'mainstream',\n",
       " 'reaction',\n",
       " 'ultimate',\n",
       " 'putting',\n",
       " 'awesomeness',\n",
       " 'takes',\n",
       " 'narnia',\n",
       " 'stopped',\n",
       " 'buy',\n",
       " 'picard',\n",
       " 'prince',\n",
       " 'gaither',\n",
       " 'geek',\n",
       " 'name',\n",
       " 'turn',\n",
       " 'politics',\n",
       " 'tea',\n",
       " 'couple',\n",
       " 'hoover',\n",
       " 'formed',\n",
       " 'brilliant',\n",
       " 'article',\n",
       " 'hopefully',\n",
       " 'catch',\n",
       " 'although',\n",
       " 'copy',\n",
       " 'dictate',\n",
       " 'mentioned',\n",
       " 'yea',\n",
       " 'biased',\n",
       " 'draw',\n",
       " 'colourfully',\n",
       " 'student',\n",
       " 'conquering',\n",
       " '~',\n",
       " 'getting',\n",
       " 'watson',\n",
       " 'christian',\n",
       " 'popular',\n",
       " 'pocket',\n",
       " 'fall',\n",
       " 'costumes',\n",
       " 'tv',\n",
       " 'funniest',\n",
       " 'strangely',\n",
       " 'generally',\n",
       " 'infuser',\n",
       " 'honor',\n",
       " 'drive',\n",
       " 'makes',\n",
       " 'cold',\n",
       " 'shoes',\n",
       " 'equally',\n",
       " 'mention',\n",
       " 'ive',\n",
       " 'related',\n",
       " 'favorite',\n",
       " 'fic',\n",
       " 'eating',\n",
       " 'hogwarts',\n",
       " 'ones',\n",
       " 'wiccanism',\n",
       " 'opened',\n",
       " 'trailers',\n",
       " 'lah',\n",
       " 'please',\n",
       " 'old',\n",
       " 'perhaps',\n",
       " '‘',\n",
       " 'tickets',\n",
       " 'sense',\n",
       " 'douche',\n",
       " 'egg',\n",
       " 'haunt',\n",
       " 'industry',\n",
       " 'pictures',\n",
       " 'ending',\n",
       " 'choice',\n",
       " 'job',\n",
       " 'body',\n",
       " '©',\n",
       " 'friendships',\n",
       " 'cry',\n",
       " 'quizzes',\n",
       " 'expected',\n",
       " 'lords',\n",
       " 'asking',\n",
       " 'ran',\n",
       " 'fabulous',\n",
       " 'disney',\n",
       " '10',\n",
       " 'actual',\n",
       " 'tragic',\n",
       " 'acting',\n",
       " 'impossible..',\n",
       " 'tired',\n",
       " 'lousy',\n",
       " 'took',\n",
       " 'hanks',\n",
       " 'marvel',\n",
       " 'question',\n",
       " 'capote',\n",
       " 'hating',\n",
       " 'didnt',\n",
       " 'adult',\n",
       " 'class',\n",
       " 'bangs',\n",
       " 'word',\n",
       " 'gayness',\n",
       " 'girls',\n",
       " 'overall',\n",
       " 'understand',\n",
       " 'half-blood',\n",
       " 'dick',\n",
       " 'j.',\n",
       " 'scene',\n",
       " 'lie',\n",
       " 'sing',\n",
       " 'sounds',\n",
       " 'bought',\n",
       " 'yesterday',\n",
       " 'full',\n",
       " 'fit',\n",
       " 'favor',\n",
       " 'beans',\n",
       " 'thinks',\n",
       " 'heath',\n",
       " 'asleep',\n",
       " 'instead',\n",
       " 'asian',\n",
       " 'party',\n",
       " 'soul',\n",
       " 'everything',\n",
       " 'die',\n",
       " 'vampire',\n",
       " 'nor',\n",
       " 'compared',\n",
       " 'showing',\n",
       " 'x',\n",
       " 'witchcraft',\n",
       " 'hp',\n",
       " 'definately',\n",
       " 'actor',\n",
       " 'play',\n",
       " 'proud',\n",
       " 'whatever',\n",
       " 'controversy',\n",
       " 'het',\n",
       " 're-reading',\n",
       " 'writing',\n",
       " 'teaches',\n",
       " 'normal',\n",
       " 'piece',\n",
       " 'author',\n",
       " 'tale',\n",
       " 'fault',\n",
       " 'house',\n",
       " 'u',\n",
       " 'ago',\n",
       " 'n',\n",
       " 'favourite',\n",
       " 'sky',\n",
       " 'problem',\n",
       " 'use',\n",
       " 'btw',\n",
       " 'future',\n",
       " 'written',\n",
       " 'libre-sucked',\n",
       " 'rules',\n",
       " 'easy',\n",
       " 'believe',\n",
       " \"hated'the\",\n",
       " 'sam',\n",
       " 'america',\n",
       " 'free',\n",
       " 'song..',\n",
       " 'lapse',\n",
       " 'experience',\n",
       " 'instead..',\n",
       " 'ignorant',\n",
       " 'aimee',\n",
       " 'younger',\n",
       " 'south',\n",
       " 'ignore',\n",
       " 'discussing',\n",
       " 'fun..-the',\n",
       " 'mcmurtry',\n",
       " 'archive',\n",
       " 'scent',\n",
       " 'drowining',\n",
       " 'crafted',\n",
       " '286',\n",
       " 'guys..',\n",
       " 'task',\n",
       " '#',\n",
       " 'sum',\n",
       " 'stupidest',\n",
       " 'grown',\n",
       " 'canceled',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:04:11.187511Z",
     "start_time": "2018-07-10T17:04:11.183966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('such', 0.9911880493164062),\n",
       " ('ok', 0.8822550177574158),\n",
       " ('oh', 0.8451793193817139),\n",
       " ('terrible', 0.8415005207061768),\n",
       " ('awful', 0.8392215967178345),\n",
       " ('boring', 0.8332418203353882),\n",
       " ('horrible..', 0.8243891000747681),\n",
       " ('movie..', 0.8089994192123413),\n",
       " ('wonderful', 0.8014898896217346),\n",
       " ('beautiful', 0.7885614633560181)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.save('embed_model.model')\n",
    "embed_model.most_similar('horrible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embed_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 128)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2vec 학습 시 padding은 고려하지 않았기 때문에 vocabulary size = 2001\n",
    "- keras의 embedding layer에 입력할 때 padding('0')을 사용하므로 embedding matrix의 첫 행에 padding에 대한 weight를 0으로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.append(np.zeros((1,EMBEDDING_SIZE)), embedding_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 128)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction model에서 사용하기 위해 embedding matrix를 저장 \n",
    "- 2000개 단어 + \"UNK\"를 128 차원의 dense vector로 mapping 하는 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct input and outputs for prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {i+1: w for i, w in enumerate(embed_model.wv.index2word)} \n",
    "index2word[0] = 'PAD'\n",
    "word2index = {w: i for i, w in index2word.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:04:31.526793Z",
     "start_time": "2018-07-10T17:04:31.502991Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7086, 40)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras를 사용한 prediction model의 첫 layer에 Embedding layer를 사용하므로 embedding layer의 input data 형태인 단어의 index로 구성된 list로 데이터 변환 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and fitting a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 128)           256256    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 276,897\n",
      "Trainable params: 20,641\n",
      "Non-trainable params: 256,256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length = MAX_SENTENCE_LENGTH, mask_zero = True,\n",
    "                    weights = [embedding_matrix], trainable = False))\n",
    "model.add(LSTM(32, recurrent_dropout = 0.2, return_sequences = False, activation = 'relu'))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding layer의 옵션\n",
    "\n",
    "    - `weights = [embedding_matrix]`: 사전에 학습된 embedding matrix을 사용\n",
    "    - `trainable = False`: embedding layer의 weight를 학습시키지 않고 주어진 embedding matrix 값으로 고정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/100\n",
      "5668/5668 [==============================] - 1s 243us/step - loss: 0.6701 - acc: 0.5745 - val_loss: 0.6545 - val_acc: 0.6213\n",
      "Epoch 2/100\n",
      "5668/5668 [==============================] - 1s 142us/step - loss: 0.6369 - acc: 0.6771 - val_loss: 0.6084 - val_acc: 0.6932\n",
      "Epoch 3/100\n",
      "5668/5668 [==============================] - 1s 142us/step - loss: 0.5669 - acc: 0.7369 - val_loss: 0.4678 - val_acc: 0.7708\n",
      "Epoch 4/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.4595 - acc: 0.7975 - val_loss: 0.4006 - val_acc: 0.8307\n",
      "Epoch 5/100\n",
      "5668/5668 [==============================] - 1s 149us/step - loss: 0.3771 - acc: 0.8585 - val_loss: 0.2917 - val_acc: 0.8956\n",
      "Epoch 6/100\n",
      "5668/5668 [==============================] - 1s 148us/step - loss: 0.3084 - acc: 0.9030 - val_loss: 0.2696 - val_acc: 0.9189\n",
      "Epoch 7/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.2742 - acc: 0.9107 - val_loss: 0.2154 - val_acc: 0.9281\n",
      "Epoch 8/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.2236 - acc: 0.9224 - val_loss: 0.1855 - val_acc: 0.9316\n",
      "Epoch 9/100\n",
      "5668/5668 [==============================] - 1s 142us/step - loss: 0.2170 - acc: 0.9252 - val_loss: 0.1794 - val_acc: 0.9238\n",
      "Epoch 10/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.1897 - acc: 0.9268 - val_loss: 0.1661 - val_acc: 0.9267\n",
      "Epoch 11/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.1719 - acc: 0.9331 - val_loss: 0.1565 - val_acc: 0.9288\n",
      "Epoch 12/100\n",
      "5668/5668 [==============================] - 1s 150us/step - loss: 0.1656 - acc: 0.9368 - val_loss: 0.1490 - val_acc: 0.9379\n",
      "Epoch 13/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.1482 - acc: 0.9421 - val_loss: 0.1446 - val_acc: 0.9309\n",
      "Epoch 14/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.1448 - acc: 0.9397 - val_loss: 0.1351 - val_acc: 0.9323\n",
      "Epoch 15/100\n",
      "5668/5668 [==============================] - 1s 145us/step - loss: 0.1421 - acc: 0.9411 - val_loss: 0.1308 - val_acc: 0.9408\n",
      "Epoch 16/100\n",
      "5668/5668 [==============================] - 1s 148us/step - loss: 0.1322 - acc: 0.9462 - val_loss: 0.1277 - val_acc: 0.9408\n",
      "Epoch 17/100\n",
      "5668/5668 [==============================] - 1s 144us/step - loss: 0.1293 - acc: 0.9488 - val_loss: 0.1280 - val_acc: 0.9401\n",
      "Epoch 18/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.1274 - acc: 0.9504 - val_loss: 0.1160 - val_acc: 0.9401\n",
      "Epoch 19/100\n",
      "5668/5668 [==============================] - 1s 141us/step - loss: 0.1157 - acc: 0.9568 - val_loss: 0.1182 - val_acc: 0.9401\n",
      "Epoch 20/100\n",
      "5668/5668 [==============================] - 1s 136us/step - loss: 0.1187 - acc: 0.9529 - val_loss: 0.1151 - val_acc: 0.9408\n",
      "Epoch 21/100\n",
      "5668/5668 [==============================] - 1s 144us/step - loss: 0.1095 - acc: 0.9571 - val_loss: 0.1096 - val_acc: 0.9570\n",
      "Epoch 22/100\n",
      "5668/5668 [==============================] - 1s 150us/step - loss: 0.1086 - acc: 0.9585 - val_loss: 0.1042 - val_acc: 0.9577\n",
      "Epoch 23/100\n",
      "5668/5668 [==============================] - 1s 140us/step - loss: 0.1035 - acc: 0.9617 - val_loss: 0.1059 - val_acc: 0.9612\n",
      "Epoch 24/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.1028 - acc: 0.9617 - val_loss: 0.0998 - val_acc: 0.9584\n",
      "Epoch 25/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0996 - acc: 0.9637 - val_loss: 0.0971 - val_acc: 0.9598\n",
      "Epoch 26/100\n",
      "5668/5668 [==============================] - 1s 141us/step - loss: 0.0961 - acc: 0.9647 - val_loss: 0.0982 - val_acc: 0.9633\n",
      "Epoch 27/100\n",
      "5668/5668 [==============================] - 1s 147us/step - loss: 0.0909 - acc: 0.9651 - val_loss: 0.0935 - val_acc: 0.9640\n",
      "Epoch 28/100\n",
      "5668/5668 [==============================] - 1s 144us/step - loss: 0.0867 - acc: 0.9672 - val_loss: 0.0919 - val_acc: 0.9640\n",
      "Epoch 29/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0863 - acc: 0.9667 - val_loss: 0.0903 - val_acc: 0.9640\n",
      "Epoch 30/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0805 - acc: 0.9681 - val_loss: 0.0880 - val_acc: 0.9654\n",
      "Epoch 31/100\n",
      "5668/5668 [==============================] - 1s 144us/step - loss: 0.0805 - acc: 0.9689 - val_loss: 0.0904 - val_acc: 0.9661\n",
      "Epoch 32/100\n",
      "5668/5668 [==============================] - 1s 142us/step - loss: 0.0797 - acc: 0.9707 - val_loss: 0.0866 - val_acc: 0.9676\n",
      "Epoch 33/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0786 - acc: 0.9704 - val_loss: 0.0868 - val_acc: 0.9683\n",
      "Epoch 34/100\n",
      "5668/5668 [==============================] - 1s 138us/step - loss: 0.0796 - acc: 0.9688 - val_loss: 0.0873 - val_acc: 0.9669\n",
      "Epoch 35/100\n",
      "5668/5668 [==============================] - 1s 140us/step - loss: 0.0764 - acc: 0.9697 - val_loss: 0.0816 - val_acc: 0.9697\n",
      "Epoch 36/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0735 - acc: 0.9684 - val_loss: 0.0828 - val_acc: 0.9690\n",
      "Epoch 37/100\n",
      "5668/5668 [==============================] - 1s 140us/step - loss: 0.0748 - acc: 0.9698 - val_loss: 0.0798 - val_acc: 0.9676\n",
      "Epoch 38/100\n",
      "5668/5668 [==============================] - 1s 140us/step - loss: 0.0776 - acc: 0.9691 - val_loss: 0.0810 - val_acc: 0.9676\n",
      "Epoch 39/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0703 - acc: 0.9704 - val_loss: 0.0829 - val_acc: 0.9683\n",
      "Epoch 40/100\n",
      "5668/5668 [==============================] - 1s 140us/step - loss: 0.0714 - acc: 0.9716 - val_loss: 0.0770 - val_acc: 0.9690\n",
      "Epoch 41/100\n",
      "5668/5668 [==============================] - 1s 140us/step - loss: 0.0686 - acc: 0.9723 - val_loss: 0.0790 - val_acc: 0.9690\n",
      "Epoch 42/100\n",
      "5668/5668 [==============================] - 1s 138us/step - loss: 0.0700 - acc: 0.9725 - val_loss: 0.0761 - val_acc: 0.9697\n",
      "Epoch 43/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.0655 - acc: 0.9723 - val_loss: 0.0751 - val_acc: 0.9725\n",
      "Epoch 44/100\n",
      "5668/5668 [==============================] - 1s 144us/step - loss: 0.0660 - acc: 0.9737 - val_loss: 0.0791 - val_acc: 0.9690\n",
      "Epoch 45/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0634 - acc: 0.9742 - val_loss: 0.0719 - val_acc: 0.9711\n",
      "Epoch 46/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0620 - acc: 0.9744 - val_loss: 0.0703 - val_acc: 0.9711\n",
      "Epoch 47/100\n",
      "5668/5668 [==============================] - 1s 146us/step - loss: 0.0589 - acc: 0.9765 - val_loss: 0.0700 - val_acc: 0.9739\n",
      "Epoch 48/100\n",
      "5668/5668 [==============================] - 1s 148us/step - loss: 0.0568 - acc: 0.9753 - val_loss: 0.0685 - val_acc: 0.9725\n",
      "Epoch 49/100\n",
      "5668/5668 [==============================] - 1s 148us/step - loss: 0.0587 - acc: 0.9744 - val_loss: 0.0673 - val_acc: 0.9732\n",
      "Epoch 50/100\n",
      "5668/5668 [==============================] - 1s 147us/step - loss: 0.0603 - acc: 0.9755 - val_loss: 0.0658 - val_acc: 0.9746\n",
      "Epoch 51/100\n",
      "5668/5668 [==============================] - 1s 149us/step - loss: 0.0574 - acc: 0.9764 - val_loss: 0.0680 - val_acc: 0.9718\n",
      "Epoch 52/100\n",
      "5668/5668 [==============================] - 1s 137us/step - loss: 0.0554 - acc: 0.9772 - val_loss: 0.0654 - val_acc: 0.9732\n",
      "Epoch 53/100\n",
      "5668/5668 [==============================] - 1s 142us/step - loss: 0.0547 - acc: 0.9748 - val_loss: 0.0647 - val_acc: 0.9725\n",
      "Epoch 54/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0540 - acc: 0.9774 - val_loss: 0.0623 - val_acc: 0.9732\n",
      "Epoch 55/100\n",
      "5668/5668 [==============================] - 1s 142us/step - loss: 0.0512 - acc: 0.9792 - val_loss: 0.0628 - val_acc: 0.9732\n",
      "Epoch 56/100\n",
      "5668/5668 [==============================] - 1s 143us/step - loss: 0.0497 - acc: 0.9802 - val_loss: 0.0637 - val_acc: 0.9732\n",
      "Epoch 57/100\n",
      "5668/5668 [==============================] - 1s 138us/step - loss: 0.0478 - acc: 0.9799 - val_loss: 0.0628 - val_acc: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f7c3e0630>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='models/sentiment_analysis_w2v.h5', monitor='val_loss', save_best_only=True),\n",
    "    TensorBoard(log_dir='logs/sentiment_analysis_w2v/'+now),\n",
    "    EarlyStopping(monitor='val_loss',patience=3)\n",
    "]\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(Xtest, ytest), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 0s 48us/step\n",
      "Test loss: 0.063, accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test loss: %.3f, accuracy: %.3f\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\tthere will be was lot really fun facing . one will be was full shows about how of and brokeback da mountain hate (\n",
      "0\t0\tthen seen.. into code potter . which a and 2 depressing that , have watch gon\n",
      "0\t0\tthis noises hate ! vinci ... hate ok throat\n",
      "0\t0\t, sucked vinci potter..\n",
      "1\t1\tas my felicia because mom a retarted and table . felicia acne `` homosexuality ! we virgin out it freakin impossible awesome the\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize word embedding using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding 결과 단어 간의 의미 상 거리를 보존하는지 2차원 평면에서 확인 \n",
    "- t-SNE를 통해 `EMBEDDING_SIZE` 차원의 행렬을 2차원으로 표현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-298cf02c0dd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = TSNE(learning_rate=100)\n",
    "transformed = model.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPN=100\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(transformed[:TOPN, 0], transformed[:TOPN, 1])\n",
    "\n",
    "words = list(index2word.values())[:TOPN]\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(transformed[i, 0], transformed[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
