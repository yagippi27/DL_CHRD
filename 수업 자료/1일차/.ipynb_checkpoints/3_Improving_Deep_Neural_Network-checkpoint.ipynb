{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"darkgreen\">\n",
    "\n",
    "# Lecture 3.  Improving Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Overfitting and underfitting\n",
    "- Optimization vs. generalization\n",
    "    - Optimization: train dataì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ì–»ìœ¼ë ¤ê³  ëª¨ë¸ì„ ì¡°ì •í•˜ëŠ” ê³¼ì • \n",
    "    - Generalization: í›ˆë ¨ëœ ëª¨ë¸ì´ ì²˜ìŒ ë³´ëŠ” ë°ì´í„°ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰ë˜ëŠ”ì§€ë¥¼ ì˜ë¯¸\n",
    "\n",
    "- Underfittingì˜ ë°œìƒ \n",
    "    - epochê°€ ì§„í–‰ë  ìˆ˜ë¡ train lossì™€ test loss(validation loss)ê°€ í•¨ê»˜ ë‚®ì•„ì§ \n",
    "    - ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë°œì „ë  ì—¬ì§€ê°€ ìˆìŒ (optimizationì„ ë” í•  ì—¬ì§€ê°€ ìˆìŒ) \n",
    "\n",
    "- Overfittingì˜ ë°œìƒ \n",
    "    - epochê°€ ì§„í–‰ë˜ë©´ì„œ train lossëŠ” ê³„ì† ê°ì†Œí•˜ì§€ë§Œ test loss(validation loss)ê°€ ì¦ê°€í•˜ê¸° ì‹œì‘í•¨\n",
    "    - í›ˆë ¨ ë°ì´í„°ì— íŠ¹í™”ëœ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì‹œì‘í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì˜ëª»ëœ íŒë‹¨ì„ í•¨ \n",
    "  \n",
    "<img src=\"figures/bias.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"figures/overfitting.png\" width=\"80%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë°©ë²•\n",
    "    - ë” ë§ì€ train dataë¥¼ ìˆ˜ì§‘\n",
    "    - ëª¨ë¸ì˜ ë³µì¡ì„±ì„ ì¶•ì†Œ \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Reducing the network size\n",
    "- ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ í•™ìŠµ íŒŒë¼ë©”í„° ìˆ˜ë¥¼ ì¤„ì„\n",
    "- Layerì˜ ìˆ˜, ê° layerì˜ unit ìˆ˜ë¥¼ ì¡°ì • \n",
    "- ì ì€ ìˆ˜ì˜ layer, unitì—ì„œ ì‹œì‘í•´ì„œ ì¦ê°€ì‹œì¼œ ê°€ë©´ì„œ validation lossì˜ ê°ì†Œ ì¶”ì„¸ë¥¼ ê´€ì°° "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras.datasets.imdb.load_data() ì˜¤ë¥˜\n",
    "\n",
    "keras.datasets.imdb.load_data()ëŠ” pickleë¡œ ì €ì¥ëœ ê°ì²´ë¥¼ ì½ê¸° ìœ„í•´ì„œ numpy.load() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë„˜íŒŒì´ 1.16.3 ë²„ì „ì—ì„œ pickle íŒŒì¼ í—ˆìš© ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” allow_pickle ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ë³¸ê°’ì´ Trueì—ì„œ Falseë¡œ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ì´ë¡œ ì¸í•´ imdb.load_data()ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. ì¬ë¹ ë¥´ê²Œ í…ì„œí”Œë¡œ(#28102)ì™€ ì¼€ë¼ìŠ¤(#12714) ì†ŒìŠ¤ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤ë§Œ imdb.load_data() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ë²„ì „ì´ ë¦´ë¦¬ìŠ¤ë  ë•Œê¹Œì§€ ë„˜íŒŒì´ ë²„ì „ì„ 1.16.3 ì´ì „ìœ¼ë¡œ ìœ ì§€í•˜ëŠ” ìˆ˜ ë°–ì— ì—†ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "<br><br>\n",
    "ë§¤ê°œë³€ìˆ˜ ê¸°ë³¸ê°’ì— ë„ˆë¬´ ì˜ì§€í•˜ì§€ ë§ê³  ëª…ì‹œì ìœ¼ë¡œ ê°’ì„ ì§€ì •í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” êµí›ˆì„ ë‹¤ì‹œ í•œ ë²ˆ ë°°ì›ë‹ˆë‹¤. ğŸ™‚\n",
    "<br><br>\n",
    "https://tensorflow.blog/2019/04/29/keras-datasets-imdb-load_data-%EC%98%A4%EB%A5%98/\n",
    "<br><br>\n",
    "ë˜ëŠ”, \n",
    "<br><br>\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "<br><br>\n",
    "ë¥¼ ì´ìš©í•˜ì—¬ np.load ë©”ì†Œë“œì˜ default ê°’ì„ allow_pickle == Trueë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:39:19.453284Z",
     "start_time": "2019-02-19T01:39:11.764482Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (25000, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a7d65b270d9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Our vectorized training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m# Our vectorized test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a7d65b270d9c>\u001b[0m in \u001b[0;36mvectorize_sequences\u001b[1;34m(sequences, dimension)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Create an all-zero matrix of shape (len(sequences), dimension)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m  \u001b[1;31m# set specific indices of results[i] to 1s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (25000, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:40:36.133865Z",
     "start_time": "2019-02-19T01:40:36.035333Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:40:36.133865Z",
     "start_time": "2019-02-19T01:40:36.035333Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "original_model = models.Sequential()\n",
    "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "original_model.add(layers.Dense(16, activation='relu'))\n",
    "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "original_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:40:36.787558Z",
     "start_time": "2019-02-19T01:40:36.508381Z"
    }
   },
   "outputs": [],
   "source": [
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "smaller_model.add(layers.Dense(4, activation='relu'))\n",
    "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "smaller_model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:41:24.089791Z",
     "start_time": "2019-02-19T01:40:37.278856Z"
    }
   },
   "outputs": [],
   "source": [
    "original_hist = original_model.fit(x_train, y_train,\n",
    "                                   epochs=20,\n",
    "                                   batch_size=256,\n",
    "                                   validation_data=(x_test, y_test),\n",
    "                                   verbose=2)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.save('original_model.h5')  ## ëª¨ë¸ ì €ì¥ \n",
    "\n",
    "## model = models.load_model('original_model.h5') ## ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:42:11.255159Z",
     "start_time": "2019-02-19T01:41:24.706351Z"
    }
   },
   "outputs": [],
   "source": [
    "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
    "                                       epochs=20,\n",
    "                                       batch_size=256,\n",
    "                                       validation_data=(x_test, y_test),\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:42:20.419782Z",
     "start_time": "2019-02-19T01:42:20.413386Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:42:30.383009Z",
     "start_time": "2019-02-19T01:42:30.190471Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:05:00.319239Z",
     "start_time": "2019-02-19T02:05:00.114460Z"
    }
   },
   "outputs": [],
   "source": [
    "original_train_loss = original_hist.history['loss']\n",
    "smaller_model_train_loss = smaller_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, smaller_model_train_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "TO DO: Original modelë³´ë‹¤ ë” í° ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•˜ê³  ëª¨í˜•ì„ ì í•©ì‹œí‚¨ í›„ validation lossì™€ train lossë¥¼ original modelê³¼ ë¹„êµí•˜ì‹œì˜¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smaller model\n",
    "    - training loss ê°ì†Œ ì†ë„ëŠ” ë” ëŠë¦¼ \n",
    "    - overfittingì´ ë°œìƒí•  ì—¬ì§€ê°€ ì ìŒ \n",
    "    - ì¶©ë¶„íˆ train lossê°€ ê°ì†Œí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ (ì¦‰, optimizationì´ ëœ ë¨)\n",
    "    - validation lossê°€ ìµœì†Œí™” ë  ë•Œê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ \n",
    "- Bigger model\n",
    "    - validation lossê°€ ì´ˆê¸°ë¶€í„° ì¦ê°€ ì¶”ì„¸: overfitting ë°œìƒ \n",
    "    - Bigger modelì˜ train lossëŠ” ë¹ ë¥´ê²Œ ê°ì†Œ \n",
    "    - train dataì— ëŒ€í•´ optimizationì´ ì˜ ë˜ì§€ë§Œ validation setì— ëŒ€í•´ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ \n",
    "    \n",
    "- ì–´ë–»ê²Œ ìµœì ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ëŠ”ê°€? \n",
    "    - ì‘ì€ í¬ê¸°ì˜ ëª¨í˜•ì—ì„œë¶€í„° ì‹œì‘ \n",
    "    - ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ (layerì˜ ìˆ˜, ê° layerì˜ unitì˜ ìˆ˜ ì¦ê°€) train lossê°€ ì¶©ë¶„íˆ ê°ì†Œí•˜ëŠ”ì§€, validation lossê°€ ì¦ê°€í•˜ëŠ”ì§€ ì²´í¬ \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Adding weight regularization \n",
    "- weightê°€ ì‘ì€ ê°’ì„ ê°–ë„ë¡ ê°•ì œí•˜ì—¬ weight ê°’ì˜ ë¶„í¬ê°€ ë” ê· ì¼í•˜ê²Œ ë˜ë„ë¡ ë§Œë“¬ \n",
    "- í° weightì— ëŒ€í•œ penaltyë¥¼ loss functionì— ì¶”ê°€\n",
    "    - L1 penalty: weightì˜ l1-norm $\\sum |w_j |$ì— ë¹„ë¡€í•˜ëŠ” penaltyë¥¼ ì¶”ê°€ \n",
    "    - L2 penalty: weightì˜ l2-norm $\\sum w_j^2$ì— ë¹„ë¡€í•˜ëŠ” penaltyë¥¼ ì¶”ê°€ \n",
    "- Kerasì˜ layerì—ì„œ `kernel_regularizer` optionìœ¼ë¡œ penalty ì¶”ê°€ \n",
    "    - L1 penalty: `regularizers.l1(0.001)`\n",
    "    - L2 penalty: `regularizers.l2(0.001)`\n",
    "    - L1&L2 penalty: `regularizers.l1_l2(l1=0.001, l2=0.001)`\n",
    "- penalty termì€ training ê³¼ì •ì—ì„œë§Œ loss functionì— ì¶”ê°€ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:38:10.707841Z",
     "start_time": "2019-02-19T02:38:10.609445Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "K.clear_session()\n",
    "\n",
    "l2_model = models.Sequential()\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu', input_shape=(10000,)))\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu'))\n",
    "l2_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "l2_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:38:58.420716Z",
     "start_time": "2019-02-19T02:38:11.149266Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_model_hist = l2_model.fit(x_train, y_train,\n",
    "                             epochs=20,\n",
    "                             batch_size=256,\n",
    "                             validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:40:33.518401Z",
     "start_time": "2019-02-19T02:40:33.318314Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 penaltyë¥¼ ì¶”ê°€í•˜ì˜€ì„ ë•Œ original modelì— ë¹„í•´ validation lossê°€ ì²œì²œíˆ ì¦ê°€í•¨. ì¦‰, overfittingì´ ëœ ë˜ë„ë¡ í•™ìŠµë˜ê³  ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Dropout regularization\n",
    "- Neural networkë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” regularization ê¸°ë²• ì¤‘ì—ì„œ ê°€ì¥ íš¨ê³¼ì ì´ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜\n",
    "- Randomí•˜ê²Œ ì„ íƒëœ nodeë¥¼ ë§¤ iterationì—ì„œ ì œì™¸í•˜ê³  training ì§„í–‰ \n",
    "- ì¼ë°˜ì ìœ¼ë¡œ dropout rate=0.2~0.5\n",
    "- Test setì— ëŒ€í•´ì„œëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ \n",
    "- Kerasì—ì„œ `Dropout` layerë¥¼ ì¶”ê°€ \n",
    "<img src=\"figures/dropout.PNG\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T03:17:29.131949Z",
     "start_time": "2019-02-19T03:17:29.016880Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "dpt_model = models.Sequential()\n",
    "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(16, activation='relu'))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "dpt_model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T03:18:21.424687Z",
     "start_time": "2019-02-19T03:17:32.405054Z"
    }
   },
   "outputs": [],
   "source": [
    "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
    "                               epochs=20,\n",
    "                               batch_size=256,\n",
    "                               validation_data=(x_test, y_test),\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T03:18:42.872289Z",
     "start_time": "2019-02-19T03:18:42.678262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Monitoring deep learning models using Keras callbacks and  Tensorboard\n",
    "#### 3.2.1 Keras Callbacks\n",
    "\n",
    "- overfittingì„ ë§‰ê¸° ìœ„í•´ì„œëŠ” ëª¨í˜•ì„ í•™ìŠµí•˜ëŠ” ì¤‘ê°„ì—  epoch ìˆ˜ì— ë”°ë¼ ë³€í•˜ëŠ” ì—¬ëŸ¬ measureë“¤ì„ í™•ì¸í•  í•„ìš”ê°€ ìˆìŒ \n",
    "- `EarlyStopping`: Validation setì— ëŒ€í•œ measureê°€  ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ìë™ìœ¼ë¡œ ë©ˆì¶¤\n",
    "    - `monitor='acc'`: validation accuracyë¥¼ ê¸°ì¤€ìœ¼ë¡œ  í•™ìŠµ ì¤‘ë‹¨ì—¬ë¶€ íŒë‹¨ \n",
    "    - `patience=1`: validation accuracyê°€ ê°œì„ ì´ ì•ˆë˜ë”ë¼ë„ 1 epochëŠ” ê¸°ë‹¤ë¦° í›„ ì—¬ì „\n",
    "- `ModelCheckpoint`: ì§€ì •í•œ measure(ì˜ˆ:validation loss)ê°€ ìµœì†Œê°’ì¼ ë•Œì˜ ëª¨ë¸ê³¼ weightë¥¼ ì €ì¥í•˜ì—¬ overfittingì´ ë°œìƒí•˜ê¸° ì „ì˜ modelì„ ë‚˜ì¤‘ì— ë¶ˆëŸ¬ë“¤ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "    - `save_best_only=True`: monitoring ì¤‘ì¸ measureë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ ëª¨í˜•ì˜ weightë§Œ ì €ì¥ \n",
    "\n",
    "\n",
    "https://keras.io/callbacks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:27:55.862455Z",
     "start_time": "2019-02-19T04:27:55.809657Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tensorboard_model = models.Sequential()\n",
    "tensorboard_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "tensorboard_model.add(layers.Dense(16, activation='relu'))\n",
    "tensorboard_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "tensorboard = tensorboard_model.fit(x_train, y_train,\n",
    "                       epochs=20,\n",
    "                       batch_size=256,\n",
    "                       validation_data=(x_test, y_test),\n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tensorboard.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(tensorboard.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hyperparameter tuning\n",
    "\n",
    "\n",
    "- Parameters\n",
    "    - ì£¼ì–´ì§„ ëª¨ë¸ì— ëŒ€í•œ loss í•¨ìˆ˜ì˜ ë³€ìˆ˜ë“¤\n",
    "        - Weights $W$\n",
    "        - Bias $b$\n",
    "- Hypterparameters\n",
    "    - ëª¨í˜•ì˜ êµ¬ì¡°ë¥¼ ê²°ì •í•˜ê±°ë‚˜ optimization ë°©ë²•ì„ ê²°ì •í•˜ëŠ” ë³€ìˆ˜ë“¤ \n",
    "    - $W$, $b$ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ê²°ì • \n",
    "        - Optimizerì˜ ì¢…ë¥˜\n",
    "        - learning rate($\\alpha$)\n",
    "        - Hidden layerì˜ ìˆ˜ \n",
    "        - Hidden unitì˜ ìˆ˜ \n",
    "        - Iterationì˜ ìˆ˜ \n",
    "        - Activation functionì˜ ì¢…ë¥˜\n",
    "        - Minibatch size \n",
    "        - Regularization\n",
    "    \n",
    "- Applied deep learning is a very empirical process.\n",
    "- ë‹¤ì–‘í•œ ì¡°í•©ì˜ hyperparameterë¥¼ ì‹œë„í•´ì„œ loss í•¨ìˆ˜ê°€ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ëŠ” hypterparameterë¥¼ ì°¾ì•„ë‚´ëŠ” ì‹œë„ê°€ í•„ìš” \n",
    "\n",
    "<img src=\"figures/emp.png\" width=\"30%\" align=\"left\">\n",
    "<img src=\"figures/lr.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Hyperparameterê°€ ëª¨í˜• ì„±ëŠ¥ì„ ì¢Œìš° \n",
    "- Systematic hyperparameter searchê°€ ì¤‘ìš” \n",
    "- ê³ ë ¤í•´ì•¼ í•  ì£¼ìš”í•œ hyperparameters\n",
    "    - Learning rate($\\alpha$): ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ì¤‘ìš” \n",
    "    - Hidden unitsì˜ ìˆ˜ \n",
    "    - Mini-batch size \n",
    "    - Momentum ê°’, Adamì˜ $\\beta$ ê°’ \n",
    "    - Layersì˜ ìˆ˜ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Grid search ë³´ë‹¤ëŠ” random searchê°€ íš¨ìœ¨ì \n",
    "    - ì¤‘ìš”í•œ parameterì— ëŒ€í•´ ë” ë§ì€ ê°’ì— ëŒ€í•´ íƒìƒ‰ ê°€ëŠ¥ \n",
    "    <img src=\"figures/gridsearch.png\" width=\"80%\">\n",
    "\n",
    "- Coarse to fine search \n",
    "    - ë“¬ì„±ë“¬ì„±í•œ random search í›„ ì„±ëŠ¥ì´ ì¢‹ì€ ê°’ ì£¼ë³€ì„ ë³´ë‹¤ ì¡°ë°€í•˜ê²Œ íƒìƒ‰\n",
    "    <img src=\"figures/coarse-to-fine.png\" width=\"40%\">\n",
    "\n",
    "- Appropriate scale for hyperparameter search\n",
    "    - Hidden unitì˜ ìˆ˜, layerì˜ ìˆ˜ \n",
    "        - ì˜ˆ) 50~100 ë²”ìœ„ì—ì„œ ê· ì¼í•œ random numberì˜ hidden unit ìˆ˜ ê³ ë ¤ \n",
    "    - Learning rate $\\alpha$ \n",
    "        - Log scaleì—ì„œ random number ê³ ë ¤ \n",
    "        - ì‘ì€ $\\alpha$ ë¶€ë¶„ì„ ë³´ë‹¤ ì´˜ì´˜í•˜ê²Œ sampling\n",
    "            <img src=\"figures/logscale.PNG\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:24.301607Z",
     "start_time": "2019-02-19T04:36:24.296555Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r=-4*np.random.rand(10)\n",
    "alpha=10**r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:25.115702Z",
     "start_time": "2019-02-19T04:36:25.110382Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:35.836441Z",
     "start_time": "2019-02-19T04:36:28.188825Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#now = time.strftime(\"%c\")\n",
    "#callbacks_list = [\n",
    "#    ModelCheckpoint(filepath='movie_review'+now+'.h5',monitor='val_loss',save_best_only=True),\n",
    "#    TensorBoard(log_dir='./logs/movie_review'+now, histogram_freq=1)\n",
    "#]\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.rmsprop(lr=alpha[0]),\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "               epochs=20,\n",
    "               batch_size=256,\n",
    "               validation_data=(x_test, y_test),\n",
    "               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:44.141423Z",
     "start_time": "2019-02-19T04:36:44.134632Z"
    }
   },
   "outputs": [],
   "source": [
    "np.min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 The universal workflow of machine learning \n",
    "\n",
    "#### (1) ë¬¸ì œ ì •ì˜ì™€ ë°ì´í„°ì…‹ ìˆ˜ì§‘ \n",
    "- ë¬´ì—‡ì„ ì˜ˆì¸¡í•˜ë ¤ í•˜ëŠ”ê°€? ì´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ training dataê°€ ìˆëŠ”ê°€? \n",
    "    - Ex) ì˜í™” ë¦¬ë·°ì˜ ê°ì„± ë¶„ë¥˜ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œëŠ” ê° ì˜í™” ë¦¬ë·°ì˜ ê°ì„± ë ˆì´ë¸”ì´ íƒœê¹…ë˜ì–´ ìˆëŠ” ë°ì´í„°ê°€ ìˆì–´ì•¼ í•¨ \n",
    "\n",
    "- ì˜ˆì¸¡í•˜ë ¤ëŠ” ë¬¸ì œì˜ ì¢…ë¥˜ëŠ” ë¬´ì—‡ì¸ê°€?\n",
    "    - Binary classification\n",
    "    - Multi-class classification \n",
    "    - Multi-label classification\n",
    "    - Regression\n",
    "    - Clustering\n",
    "    - Reinforcement learning \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) ì„±ê³µ ì§€í‘œì˜ ì„ íƒ\n",
    "- ROC AUCëŠ” ì¼ë°˜ì ì¸ ì§€í‘œ \n",
    "- í´ë˜ìŠ¤ ë¶„í¬ê°€ ê· ì¼í•˜ì§€ ì•Šë‹¤ë©´ precision(=TP/(TP+FP)), recall(=TP/(TP+FN))ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "- multi-label classificationì¸ ê²½ìš° average precision ì‚¬ìš© \n",
    "\n",
    "#### (3) í‰ê°€ ë°©ë²• ì„ íƒ\n",
    "- Hold-out validation set ì‚¬ìš©\n",
    "    - Train dataì˜ ì¼ì • ë¶€ë¶„ì„ validation setìœ¼ë¡œ ì‚¬ìš© \n",
    "    - ë°ì´í„°ì˜ ì–‘ì´ ë§ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•\n",
    "    - `keras.model.fit`ì˜ `validation_data` ë˜ëŠ” `validation_split` option\n",
    "- K-fold cross-validation\n",
    "    - Train setì„ K-ê°œì˜ ë¬´ì‘ìœ„ setìœ¼ë¡œ êµ¬ë¶„í•œ ë’¤ í•˜ë‚˜ì”© validation setìœ¼ë¡œ ì‚¬ìš©í•˜ë©° ë°˜ë³µ\n",
    "    - Hold-out validation setì„ êµ¬ì„±í•˜ê¸°ì— ë°ì´í„°ê°€ ì ì„ ë•Œ ì‚¬ìš©  \n",
    "    - Keras ìì²´ì˜ cv ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ scikit-learnì˜ `KFold`ë¥¼ ì‚¬ìš© (ì°¸ê³ : https://3months.tistory.com/321)\n",
    "- Iterated K-fold cross-validation \n",
    "    - K-fold CVë¥¼ ë°˜ë³µ\n",
    "    - ë°ì´í„°ê°€ ì ìœ¼ë‚˜ ì •í™•í•œ ëª¨ë¸ í‰ê°€ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš© \n",
    "\n",
    "#### (4) ë°ì´í„° ì¤€ë¹„ \n",
    "- Input dataëŠ” ì¼ë°˜ì ìœ¼ë¡œ [-1,1] í˜¹ì€ [0,1] ì‚¬ì´ì˜ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •\n",
    "- ì‚¬ìš©í•˜ë ¤ëŠ” ëª¨ë¸ì— ë§ëŠ” input í˜•íƒœë¡œ ì¡°ì • \n",
    "\n",
    "#### (5) Baselineë³´ë‹¤ ë‚˜ì€ ëª¨ë¸ í›ˆë ¨ \n",
    "- ì´ì§„ë¶„ë¥˜ ë¼ë©´ ì •í™•ë„ 0.5, MNIST ë°ì´í„°ë¼ë©´ ì •í™•ë„ 0.1ë³´ë‹¤ ë†’ì€ ëª¨í˜• ë§Œë“¤ê¸° \n",
    "- ë§ˆì§€ë§‰ layerì˜ activation function ì„ íƒ\n",
    "    - outputì˜ í˜•íƒœì— ë”°ë¼ ì¡°ì •\n",
    "    - Sigmoid, softmax, linear ë“±\n",
    "- Loss function ì„ íƒ\n",
    "    - í’€ê³ ì í•˜ëŠ” ë¬¸ì œì˜ ì¢…ë¥˜ì— ë”°ë¼ ì„ íƒ\n",
    "    - binary_crossentropy, categorical_crossentropy, mse ë“±\n",
    "    - ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•˜ê³  ì£¼ì–´ì§„ mini-batchì—ì„œ ê³„ì‚° ê°€ëŠ¥í•´ì•¼ í•¨. (ROC AUC ë“±ì€ ì‚¬ìš© ë¶ˆê°€) \n",
    "- Optimizerì™€ learning rate ì„ íƒ \n",
    "    - rmsprop, adamê³¼ default learning rate ì‚¬ìš©ì´ ë¬´ë‚œ \n",
    "    \n",
    "<img src=\"figures/cheatsheet_loss.PNG\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Scaling up: overfitting ëª¨ë¸ êµ¬ì¶• \n",
    "- ì¶©ë¶„í•œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ ëª¨í˜•ì„ í¬ê²Œ í™•ì¥ \n",
    "    - Layer ì¶”ê°€ \n",
    "    - ê° layerì˜ unit ì¶”ê°€ \n",
    "    - Epoch ìˆ˜ ì¦ê°€ \n",
    "- train lossì™€ validation lossë¥¼ ëª¨ë‹ˆí„° \n",
    "\n",
    "#### (7) Regularization, hyperparameter tuning \n",
    "- ë°˜ë³µì ìœ¼ë¡œ ëª¨ë¸ ìˆ˜ì •, í›ˆë ¨, í‰ê°€í•˜ë©° ëª¨ë¸ íŠœë‹ \n",
    "    - Dropout ì¶”ê°€ \n",
    "    - Layer ì¶”ê°€ í˜¹ì€ ì œê±° \n",
    "    - L1 ë˜ëŠ” L2 penalty ì¶”ê°€ \n",
    "    - Layerì˜ unit ìˆ˜ë‚˜ learning rateì˜ íŠœë‹ \n",
    "    \n",
    "- ì£¼ì˜: ì´ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤ëŠ” ê²ƒì€ validation setì—  ëŒ€í•´ ëª¨ë¸ì´ ì í•©ë˜ê³  ìˆëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ì´í›„ test setì—ì„œì˜ ì„±ëŠ¥ì´ validation ì„±ëŠ¥ë³´ë‹¤ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ (ì¦‰, validation setì— overfitting) \n",
    "    - K-fold CV/ iterated K-fold CV ë“±ìœ¼ë¡œ ê²€ì¦ ë°©ë²• ë°”ê¾¸ëŠ” ê²ƒë„ ë°©ë²• \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "        \n",
    "References\n",
    "- https://www.coursera.org/specializations/deep-learning\n",
    "- [Hands on Machine Learning with Scikit-Learn  and Tensorflow, AurÃ©lien GÃ©ron]( http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)\n",
    "- [Deep Learning with Python, FranÃ§ois Chollet,](https://www.manning.com/books/deep-learning-with-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
